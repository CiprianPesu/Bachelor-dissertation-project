{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "222e4d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import AutoTokenizer, DistilBertModel\n",
    "from transformers import AdamW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4effb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce GTX 1050 Ti\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "  for i in range(torch.cuda.device_count()):\n",
    "    print(torch.cuda.get_device_name(i))\n",
    "else:\n",
    "  print(\"You are running on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef0d75c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Datasets/Cuvinte-Eliminate/train-punct-stop-1000.csv\")\n",
    "df=df.dropna()\n",
    "df=df.sample(n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f623596e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2663110</th>\n",
       "      <td>0</td>\n",
       "      <td>toy not remember ones played back 80s one rece...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2943521</th>\n",
       "      <td>0</td>\n",
       "      <td>dissapointed mount stated use 200 lost ride du...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2168511</th>\n",
       "      <td>0</td>\n",
       "      <td>book way long reviewers pointed runs steam hal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056310</th>\n",
       "      <td>2</td>\n",
       "      <td>really love series complete set like read prev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305773</th>\n",
       "      <td>2</td>\n",
       "      <td>first album first album totally different thre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment                                               text\n",
       "2663110          0  toy not remember ones played back 80s one rece...\n",
       "2943521          0  dissapointed mount stated use 200 lost ride du...\n",
       "2168511          0  book way long reviewers pointed runs steam hal...\n",
       "1056310          2  really love series complete set like read prev...\n",
       "1305773          2  first album first album totally different thre..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc3a8b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df['sentiment'].replace(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53c66935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2663110</th>\n",
       "      <td>0</td>\n",
       "      <td>toy not remember ones played back 80s one rece...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2943521</th>\n",
       "      <td>0</td>\n",
       "      <td>dissapointed mount stated use 200 lost ride du...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2168511</th>\n",
       "      <td>0</td>\n",
       "      <td>book way long reviewers pointed runs steam hal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056310</th>\n",
       "      <td>1</td>\n",
       "      <td>really love series complete set like read prev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305773</th>\n",
       "      <td>1</td>\n",
       "      <td>first album first album totally different thre...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment                                               text\n",
       "2663110          0  toy not remember ones played back 80s one rece...\n",
       "2943521          0  dissapointed mount stated use 200 lost ride du...\n",
       "2168511          0  book way long reviewers pointed runs steam hal...\n",
       "1056310          1  really love series complete set like read prev...\n",
       "1305773          1  first album first album totally different thre..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38d11fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "def tokenize_function(example):\n",
    "    tok = tokenizer(example, padding=\"max_length\", truncation=True)\n",
    "    return tok['input_ids'], tok['attention_mask']\n",
    "    \n",
    "df['input_ids'], df['attention_mask'] = zip(*df['text'].map(tokenize_function))\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f509f2b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2663110</th>\n",
       "      <td>0</td>\n",
       "      <td>toy not remember ones played back 80s one rece...</td>\n",
       "      <td>[101, 9121, 2025, 3342, 3924, 2209, 2067, 1600...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2943521</th>\n",
       "      <td>0</td>\n",
       "      <td>dissapointed mount stated use 200 lost ride du...</td>\n",
       "      <td>[101, 4487, 11488, 8400, 2098, 4057, 3090, 222...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2168511</th>\n",
       "      <td>0</td>\n",
       "      <td>book way long reviewers pointed runs steam hal...</td>\n",
       "      <td>[101, 2338, 2126, 2146, 15814, 4197, 3216, 549...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1056310</th>\n",
       "      <td>1</td>\n",
       "      <td>really love series complete set like read prev...</td>\n",
       "      <td>[101, 2428, 2293, 2186, 3143, 2275, 2066, 3191...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305773</th>\n",
       "      <td>1</td>\n",
       "      <td>first album first album totally different thre...</td>\n",
       "      <td>[101, 2034, 2201, 2034, 2201, 6135, 2367, 2093...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment                                               text  \\\n",
       "2663110          0  toy not remember ones played back 80s one rece...   \n",
       "2943521          0  dissapointed mount stated use 200 lost ride du...   \n",
       "2168511          0  book way long reviewers pointed runs steam hal...   \n",
       "1056310          1  really love series complete set like read prev...   \n",
       "1305773          1  first album first album totally different thre...   \n",
       "\n",
       "                                                 input_ids  \\\n",
       "2663110  [101, 9121, 2025, 3342, 3924, 2209, 2067, 1600...   \n",
       "2943521  [101, 4487, 11488, 8400, 2098, 4057, 3090, 222...   \n",
       "2168511  [101, 2338, 2126, 2146, 15814, 4197, 3216, 549...   \n",
       "1056310  [101, 2428, 2293, 2186, 3143, 2275, 2066, 3191...   \n",
       "1305773  [101, 2034, 2201, 2034, 2201, 6135, 2367, 2093...   \n",
       "\n",
       "                                            attention_mask  \n",
       "2663110  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2943521  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2168511  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1056310  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1305773  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b87b37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.df.iloc[idx]\n",
    "        return {\n",
    "            'input_ids' : torch.tensor(item['input_ids']).to(device),\n",
    "            'attention_mask' : torch.tensor(item['attention_mask']).to(device),\n",
    "            'labels' : torch.tensor(item['sentiment']).to(device)\n",
    "        }\n",
    "\n",
    "train_set = Dataset(df_train)\n",
    "test_set = Dataset(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dd49b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier(\n",
      "  (bert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (1): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (2): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (3): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (4): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (5): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (cls): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, out_feat=2):\n",
    "        super().__init__()\n",
    "        self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.cls = nn.Linear(768, out_feat)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    " \n",
    "        pooled_output = outputs.last_hidden_state[:,0,:] #[batch_dim, token_dim, ...] [CLS]\n",
    "        logits = self.cls(pooled_output)\n",
    "        return F.softmax(logits, dim=1)\n",
    "\n",
    "    def freeze_until_layer(self, n):\n",
    "      for name, param in self.named_parameters():\n",
    "        if str(n) in name:\n",
    "          break\n",
    "\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    def print_layers(self):\n",
    "      total_nr_w = 0\n",
    "      trainable_nr_w = 0\n",
    "      for name, param in self.named_parameters():\n",
    "        nr_w = np.prod(param.size())\n",
    "        total_nr_w += nr_w\n",
    "        if param.requires_grad:\n",
    "          trainable_nr_w += nr_w\n",
    "        print('{}\\t{}\\t\\t\\t{}'.format(param.requires_grad, nr_w ,name))\n",
    "      print('The network has {} parameters, out of which {} ({:.1f}%) are trainable.'.format(total_nr_w, trainable_nr_w, trainable_nr_w / total_nr_w * 100))\n",
    "\n",
    "model = Classifier(2).to(device) # generez o instanță a modelului\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e47f9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\t23440896\t\t\tbert.embeddings.word_embeddings.weight\n",
      "False\t393216\t\t\tbert.embeddings.position_embeddings.weight\n",
      "False\t768\t\t\tbert.embeddings.LayerNorm.weight\n",
      "False\t768\t\t\tbert.embeddings.LayerNorm.bias\n",
      "False\t589824\t\t\tbert.transformer.layer.0.attention.q_lin.weight\n",
      "False\t768\t\t\tbert.transformer.layer.0.attention.q_lin.bias\n",
      "False\t589824\t\t\tbert.transformer.layer.0.attention.k_lin.weight\n",
      "False\t768\t\t\tbert.transformer.layer.0.attention.k_lin.bias\n",
      "False\t589824\t\t\tbert.transformer.layer.0.attention.v_lin.weight\n",
      "False\t768\t\t\tbert.transformer.layer.0.attention.v_lin.bias\n",
      "False\t589824\t\t\tbert.transformer.layer.0.attention.out_lin.weight\n",
      "False\t768\t\t\tbert.transformer.layer.0.attention.out_lin.bias\n",
      "False\t768\t\t\tbert.transformer.layer.0.sa_layer_norm.weight\n",
      "False\t768\t\t\tbert.transformer.layer.0.sa_layer_norm.bias\n",
      "False\t2359296\t\t\tbert.transformer.layer.0.ffn.lin1.weight\n",
      "False\t3072\t\t\tbert.transformer.layer.0.ffn.lin1.bias\n",
      "False\t2359296\t\t\tbert.transformer.layer.0.ffn.lin2.weight\n",
      "False\t768\t\t\tbert.transformer.layer.0.ffn.lin2.bias\n",
      "False\t768\t\t\tbert.transformer.layer.0.output_layer_norm.weight\n",
      "False\t768\t\t\tbert.transformer.layer.0.output_layer_norm.bias\n",
      "False\t589824\t\t\tbert.transformer.layer.1.attention.q_lin.weight\n",
      "False\t768\t\t\tbert.transformer.layer.1.attention.q_lin.bias\n",
      "False\t589824\t\t\tbert.transformer.layer.1.attention.k_lin.weight\n",
      "False\t768\t\t\tbert.transformer.layer.1.attention.k_lin.bias\n",
      "False\t589824\t\t\tbert.transformer.layer.1.attention.v_lin.weight\n",
      "False\t768\t\t\tbert.transformer.layer.1.attention.v_lin.bias\n",
      "False\t589824\t\t\tbert.transformer.layer.1.attention.out_lin.weight\n",
      "False\t768\t\t\tbert.transformer.layer.1.attention.out_lin.bias\n",
      "False\t768\t\t\tbert.transformer.layer.1.sa_layer_norm.weight\n",
      "False\t768\t\t\tbert.transformer.layer.1.sa_layer_norm.bias\n",
      "False\t2359296\t\t\tbert.transformer.layer.1.ffn.lin1.weight\n",
      "False\t3072\t\t\tbert.transformer.layer.1.ffn.lin1.bias\n",
      "False\t2359296\t\t\tbert.transformer.layer.1.ffn.lin2.weight\n",
      "False\t768\t\t\tbert.transformer.layer.1.ffn.lin2.bias\n",
      "False\t768\t\t\tbert.transformer.layer.1.output_layer_norm.weight\n",
      "False\t768\t\t\tbert.transformer.layer.1.output_layer_norm.bias\n",
      "False\t589824\t\t\tbert.transformer.layer.2.attention.q_lin.weight\n",
      "False\t768\t\t\tbert.transformer.layer.2.attention.q_lin.bias\n",
      "False\t589824\t\t\tbert.transformer.layer.2.attention.k_lin.weight\n",
      "False\t768\t\t\tbert.transformer.layer.2.attention.k_lin.bias\n",
      "False\t589824\t\t\tbert.transformer.layer.2.attention.v_lin.weight\n",
      "False\t768\t\t\tbert.transformer.layer.2.attention.v_lin.bias\n",
      "False\t589824\t\t\tbert.transformer.layer.2.attention.out_lin.weight\n",
      "False\t768\t\t\tbert.transformer.layer.2.attention.out_lin.bias\n",
      "False\t768\t\t\tbert.transformer.layer.2.sa_layer_norm.weight\n",
      "False\t768\t\t\tbert.transformer.layer.2.sa_layer_norm.bias\n",
      "False\t2359296\t\t\tbert.transformer.layer.2.ffn.lin1.weight\n",
      "False\t3072\t\t\tbert.transformer.layer.2.ffn.lin1.bias\n",
      "False\t2359296\t\t\tbert.transformer.layer.2.ffn.lin2.weight\n",
      "False\t768\t\t\tbert.transformer.layer.2.ffn.lin2.bias\n",
      "False\t768\t\t\tbert.transformer.layer.2.output_layer_norm.weight\n",
      "False\t768\t\t\tbert.transformer.layer.2.output_layer_norm.bias\n",
      "False\t589824\t\t\tbert.transformer.layer.3.attention.q_lin.weight\n",
      "False\t768\t\t\tbert.transformer.layer.3.attention.q_lin.bias\n",
      "False\t589824\t\t\tbert.transformer.layer.3.attention.k_lin.weight\n",
      "False\t768\t\t\tbert.transformer.layer.3.attention.k_lin.bias\n",
      "False\t589824\t\t\tbert.transformer.layer.3.attention.v_lin.weight\n",
      "False\t768\t\t\tbert.transformer.layer.3.attention.v_lin.bias\n",
      "False\t589824\t\t\tbert.transformer.layer.3.attention.out_lin.weight\n",
      "False\t768\t\t\tbert.transformer.layer.3.attention.out_lin.bias\n",
      "False\t768\t\t\tbert.transformer.layer.3.sa_layer_norm.weight\n",
      "False\t768\t\t\tbert.transformer.layer.3.sa_layer_norm.bias\n",
      "False\t2359296\t\t\tbert.transformer.layer.3.ffn.lin1.weight\n",
      "False\t3072\t\t\tbert.transformer.layer.3.ffn.lin1.bias\n",
      "False\t2359296\t\t\tbert.transformer.layer.3.ffn.lin2.weight\n",
      "False\t768\t\t\tbert.transformer.layer.3.ffn.lin2.bias\n",
      "False\t768\t\t\tbert.transformer.layer.3.output_layer_norm.weight\n",
      "False\t768\t\t\tbert.transformer.layer.3.output_layer_norm.bias\n",
      "False\t589824\t\t\tbert.transformer.layer.4.attention.q_lin.weight\n",
      "False\t768\t\t\tbert.transformer.layer.4.attention.q_lin.bias\n",
      "False\t589824\t\t\tbert.transformer.layer.4.attention.k_lin.weight\n",
      "False\t768\t\t\tbert.transformer.layer.4.attention.k_lin.bias\n",
      "False\t589824\t\t\tbert.transformer.layer.4.attention.v_lin.weight\n",
      "False\t768\t\t\tbert.transformer.layer.4.attention.v_lin.bias\n",
      "False\t589824\t\t\tbert.transformer.layer.4.attention.out_lin.weight\n",
      "False\t768\t\t\tbert.transformer.layer.4.attention.out_lin.bias\n",
      "False\t768\t\t\tbert.transformer.layer.4.sa_layer_norm.weight\n",
      "False\t768\t\t\tbert.transformer.layer.4.sa_layer_norm.bias\n",
      "False\t2359296\t\t\tbert.transformer.layer.4.ffn.lin1.weight\n",
      "False\t3072\t\t\tbert.transformer.layer.4.ffn.lin1.bias\n",
      "False\t2359296\t\t\tbert.transformer.layer.4.ffn.lin2.weight\n",
      "False\t768\t\t\tbert.transformer.layer.4.ffn.lin2.bias\n",
      "False\t768\t\t\tbert.transformer.layer.4.output_layer_norm.weight\n",
      "False\t768\t\t\tbert.transformer.layer.4.output_layer_norm.bias\n",
      "True\t589824\t\t\tbert.transformer.layer.5.attention.q_lin.weight\n",
      "True\t768\t\t\tbert.transformer.layer.5.attention.q_lin.bias\n",
      "True\t589824\t\t\tbert.transformer.layer.5.attention.k_lin.weight\n",
      "True\t768\t\t\tbert.transformer.layer.5.attention.k_lin.bias\n",
      "True\t589824\t\t\tbert.transformer.layer.5.attention.v_lin.weight\n",
      "True\t768\t\t\tbert.transformer.layer.5.attention.v_lin.bias\n",
      "True\t589824\t\t\tbert.transformer.layer.5.attention.out_lin.weight\n",
      "True\t768\t\t\tbert.transformer.layer.5.attention.out_lin.bias\n",
      "True\t768\t\t\tbert.transformer.layer.5.sa_layer_norm.weight\n",
      "True\t768\t\t\tbert.transformer.layer.5.sa_layer_norm.bias\n",
      "True\t2359296\t\t\tbert.transformer.layer.5.ffn.lin1.weight\n",
      "True\t3072\t\t\tbert.transformer.layer.5.ffn.lin1.bias\n",
      "True\t2359296\t\t\tbert.transformer.layer.5.ffn.lin2.weight\n",
      "True\t768\t\t\tbert.transformer.layer.5.ffn.lin2.bias\n",
      "True\t768\t\t\tbert.transformer.layer.5.output_layer_norm.weight\n",
      "True\t768\t\t\tbert.transformer.layer.5.output_layer_norm.bias\n",
      "True\t1536\t\t\tcls.weight\n",
      "True\t2\t\t\tcls.bias\n",
      "The network has 66364418 parameters, out of which 7089410 (10.7%) are trainable.\n"
     ]
    }
   ],
   "source": [
    "model.freeze_until_layer(5)\n",
    "model.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc478e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "  # Pregatim o modalitate de stocare a datelor pentru evaluare\n",
    "  eval_outputs = []\n",
    "  true_labels = []\n",
    "  # Trecem modelul in modul train\n",
    "  model.eval()\n",
    "\n",
    "  ########### Evaluation Loop #############\n",
    "  with torch.no_grad():\n",
    "      for batch in tqdm(test_loader, total=len(test_loader)):\n",
    "          outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "\n",
    "          outputs = outputs.cpu() ## copy-ing the outputs from CUDA to CPU\n",
    "          outputs = np.argmax(outputs, axis=1)\n",
    "\n",
    "          eval_outputs += outputs.tolist()\n",
    "          true_labels += batch['labels'].tolist()\n",
    "\n",
    "  #acc = metrics.accuracy_score(true_labels, eval_outputs)\n",
    "  f1 = metrics.f1_score(true_labels, eval_outputs)\n",
    "  print(\"F1: {}\".format(f1) , end =\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8da9ce98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch 0 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 25/25 [00:37<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with train loss: 0.6736633920669556 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 7/7 [00:07<00:00,  1.02s/it]\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.7352941176470589 Running epoch 1 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 25/25 [00:36<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with train loss: 0.6031738662719727 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 7/7 [00:07<00:00,  1.03s/it]\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.7741935483870966 Running epoch 2 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 25/25 [00:38<00:00,  1.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with train loss: 0.5188163232803344 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 7/7 [00:07<00:00,  1.08s/it]\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.745945945945946 Running epoch 3 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 25/25 [00:38<00:00,  1.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with train loss: 0.4845039808750153 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 7/7 [00:07<00:00,  1.08s/it]\n",
      "  0%|                                                    | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.8058252427184465 Running epoch 4 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 25/25 [00:39<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with train loss: 0.4584741127490997 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 7/7 [00:07<00:00,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.8159203980099503 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 5e-5 # Rata de invatare\n",
    "NR_EPOCHS = 5 # Numarul de epoci\n",
    "BATCH_SIZE = 32 # Numarul de samples dintr-un batch\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Pregatim o modalitate de loggare a informatiilor din timpul antrenarii\n",
    "log_info = []\n",
    "\n",
    "# Pregatim DataLoader-ul pentru antrenare\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Pregatim DataLoader-ul pentru validare\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=32, shuffle=False)\n",
    "\n",
    "# Trecem modelul in modul train\n",
    "model.train() \n",
    "\n",
    "\n",
    "########### Training Loop #############\n",
    "\n",
    "\n",
    "min_epoch_loss = np.Inf\n",
    "last_epoch_loss = np.Inf\n",
    "# pentru fiecare epoca (1 epoca = o iteratie peste intregul set de date)\n",
    "for epoch in range(NR_EPOCHS):\n",
    "    print('Running epoch {}'.format(epoch), end =\" \")\n",
    "\n",
    "    epoch_losses = []\n",
    "    # pentru fiecare batch de BATCH_SIZE exemple din setul de date    \n",
    "    for i, batch in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "\n",
    "        # anulam gradientii deja acumulati la nivelul retelei neuronale\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # FORWARD PASS: trecem inputurile prin retea\n",
    "        outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "\n",
    "        # Calculam LOSSul dintre etichetele prezise si cele reale\n",
    "        loss = criterion(outputs, batch['labels'])\n",
    "\n",
    "        # BACKPRPAGATION: calculam gradientii propagand LOSSul in retea\n",
    "        loss.backward()\n",
    "\n",
    "        # Utilizam optimizorul pentru a modifica parametrii retelei in functie de gradientii acumulati\n",
    "        optimizer.step()\n",
    "\n",
    "        # Salvam informatii despre antrenare (in cazul nostru, salvam valoarea LOSS)\n",
    "        epoch_losses.append(loss.item()) \n",
    "    \n",
    "    this_epoch_loss = np.mean(epoch_losses)\n",
    "    log_info.append((epoch, this_epoch_loss))\n",
    "    \n",
    "    if this_epoch_loss <= min_epoch_loss:\n",
    "      min_epoch_loss = this_epoch_loss\n",
    "      print(\"Saving model with train loss: {}\".format(this_epoch_loss), end =\" \")\n",
    "      torch.save(model, \"classfication_model.pt\")\n",
    "\n",
    "    evaluate(model)\n",
    "    \n",
    "    if last_epoch_loss - this_epoch_loss < 0.01:\n",
    "      print(\"Early Stopping!\")\n",
    "      break\n",
    "\n",
    "    last_epoch_loss = this_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2eefc2e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = [x for x, loss in log_info]\n",
    "Y = [loss for x, loss in log_info]\n",
    "plt.plot(X,Y)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"LOSS\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2531efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 7/7 [00:07<00:00,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.8159203980099503 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
