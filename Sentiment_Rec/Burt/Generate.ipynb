{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "222e4d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import AutoTokenizer, DistilBertModel\n",
    "from transformers import AdamW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4effb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce GTX 1050 Ti\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "  for i in range(torch.cuda.device_count()):\n",
    "    print(torch.cuda.get_device_name(i))\n",
    "else:\n",
    "  print(\"You are running on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef0d75c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Datasets/Cuvinte-Eliminate/train-punct-stop-1000.csv\")\n",
    "df=df.dropna()\n",
    "df=df.sample(n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f623596e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>479589</th>\n",
       "      <td>2</td>\n",
       "      <td>really cds set player get ready journey like n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626331</th>\n",
       "      <td>2</td>\n",
       "      <td>although harold battles book small simply hard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346982</th>\n",
       "      <td>2</td>\n",
       "      <td>whats base black red root bark bark sesame see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1605488</th>\n",
       "      <td>0</td>\n",
       "      <td>bought product hooking found need telephone ja...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051640</th>\n",
       "      <td>2</td>\n",
       "      <td>absolutely love mobile received mail today son...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment                                               text\n",
       "479589           2  really cds set player get ready journey like n...\n",
       "626331           2  although harold battles book small simply hard...\n",
       "346982           2  whats base black red root bark bark sesame see...\n",
       "1605488          0  bought product hooking found need telephone ja...\n",
       "1051640          2  absolutely love mobile received mail today son..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc3a8b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df['sentiment'].replace(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53c66935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>479589</th>\n",
       "      <td>1</td>\n",
       "      <td>really cds set player get ready journey like n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626331</th>\n",
       "      <td>1</td>\n",
       "      <td>although harold battles book small simply hard...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346982</th>\n",
       "      <td>1</td>\n",
       "      <td>whats base black red root bark bark sesame see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1605488</th>\n",
       "      <td>0</td>\n",
       "      <td>bought product hooking found need telephone ja...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051640</th>\n",
       "      <td>1</td>\n",
       "      <td>absolutely love mobile received mail today son...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment                                               text\n",
       "479589           1  really cds set player get ready journey like n...\n",
       "626331           1  although harold battles book small simply hard...\n",
       "346982           1  whats base black red root bark bark sesame see...\n",
       "1605488          0  bought product hooking found need telephone ja...\n",
       "1051640          1  absolutely love mobile received mail today son..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38d11fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "def tokenize_function(example):\n",
    "    tok = tokenizer(example, padding=\"max_length\", truncation=True)\n",
    "    return tok['input_ids'], tok['attention_mask']\n",
    "    \n",
    "df['input_ids'], df['attention_mask'] = zip(*df['text'].map(tokenize_function))\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f509f2b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>479589</th>\n",
       "      <td>1</td>\n",
       "      <td>really cds set player get ready journey like n...</td>\n",
       "      <td>[101, 2428, 14340, 2275, 2447, 2131, 3201, 499...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626331</th>\n",
       "      <td>1</td>\n",
       "      <td>although harold battles book small simply hard...</td>\n",
       "      <td>[101, 2348, 7157, 7465, 2338, 2235, 3432, 2524...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346982</th>\n",
       "      <td>1</td>\n",
       "      <td>whats base black red root bark bark sesame see...</td>\n",
       "      <td>[101, 2054, 2015, 2918, 2304, 2417, 7117, 1128...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1605488</th>\n",
       "      <td>0</td>\n",
       "      <td>bought product hooking found need telephone ja...</td>\n",
       "      <td>[101, 4149, 4031, 8103, 2075, 2179, 2342, 7026...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051640</th>\n",
       "      <td>1</td>\n",
       "      <td>absolutely love mobile received mail today son...</td>\n",
       "      <td>[101, 7078, 2293, 4684, 2363, 5653, 2651, 2365...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment                                               text  \\\n",
       "479589           1  really cds set player get ready journey like n...   \n",
       "626331           1  although harold battles book small simply hard...   \n",
       "346982           1  whats base black red root bark bark sesame see...   \n",
       "1605488          0  bought product hooking found need telephone ja...   \n",
       "1051640          1  absolutely love mobile received mail today son...   \n",
       "\n",
       "                                                 input_ids  \\\n",
       "479589   [101, 2428, 14340, 2275, 2447, 2131, 3201, 499...   \n",
       "626331   [101, 2348, 7157, 7465, 2338, 2235, 3432, 2524...   \n",
       "346982   [101, 2054, 2015, 2918, 2304, 2417, 7117, 1128...   \n",
       "1605488  [101, 4149, 4031, 8103, 2075, 2179, 2342, 7026...   \n",
       "1051640  [101, 7078, 2293, 4684, 2363, 5653, 2651, 2365...   \n",
       "\n",
       "                                            attention_mask  \n",
       "479589   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "626331   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "346982   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1605488  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1051640  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b87b37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.df.iloc[idx]\n",
    "        return {\n",
    "            'input_ids' : torch.tensor(item['input_ids']).to(device),\n",
    "            'attention_mask' : torch.tensor(item['attention_mask']).to(device),\n",
    "            'labels' : torch.tensor(item['sentiment']).to(device)\n",
    "        }\n",
    "\n",
    "train_set = Dataset(df_train)\n",
    "test_set = Dataset(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dd49b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier(\n",
      "  (bert): DistilBertModel(\n",
      "    (embeddings): Embeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      (layer): ModuleList(\n",
      "        (0): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (1): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (2): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (3): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (4): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "        (5): TransformerBlock(\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (cls): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, out_feat=2):\n",
    "        super().__init__()\n",
    "        self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.cls = nn.Linear(768, out_feat)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    " \n",
    "        pooled_output = outputs.last_hidden_state[:,0,:] #[batch_dim, token_dim, ...] [CLS]\n",
    "        logits = self.cls(pooled_output)\n",
    "        return F.softmax(logits, dim=1)\n",
    "\n",
    "    def freeze_until_layer(self, n):\n",
    "      for name, param in self.named_parameters():\n",
    "        if str(n) in name:\n",
    "          break\n",
    "\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    def print_layers(self):\n",
    "      total_nr_w = 0\n",
    "      trainable_nr_w = 0\n",
    "      for name, param in self.named_parameters():\n",
    "        nr_w = np.prod(param.size())\n",
    "        total_nr_w += nr_w\n",
    "        if param.requires_grad:\n",
    "          trainable_nr_w += nr_w\n",
    "        print('{}\\t{}\\t\\t\\t{}'.format(param.requires_grad, nr_w ,name))\n",
    "      print('The network has {} parameters, out of which {} ({:.1f}%) are trainable.'.format(total_nr_w, trainable_nr_w, trainable_nr_w / total_nr_w * 100))\n",
    "\n",
    "model = Classifier(2).to(device) # generez o instanță a modelului\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e47f9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\t23440896\t\t\tbert.embeddings.word_embeddings.weight\n",
      "False\t393216\t\t\tbert.embeddings.position_embeddings.weight\n",
      "False\t768\t\t\tbert.embeddings.LayerNorm.weight\n",
      "False\t768\t\t\tbert.embeddings.LayerNorm.bias\n",
      "False\t589824\t\t\tbert.transformer.layer.0.attention.q_lin.weight\n",
      "False\t768\t\t\tbert.transformer.layer.0.attention.q_lin.bias\n",
      "False\t589824\t\t\tbert.transformer.layer.0.attention.k_lin.weight\n",
      "False\t768\t\t\tbert.transformer.layer.0.attention.k_lin.bias\n",
      "False\t589824\t\t\tbert.transformer.layer.0.attention.v_lin.weight\n",
      "False\t768\t\t\tbert.transformer.layer.0.attention.v_lin.bias\n",
      "False\t589824\t\t\tbert.transformer.layer.0.attention.out_lin.weight\n",
      "False\t768\t\t\tbert.transformer.layer.0.attention.out_lin.bias\n",
      "False\t768\t\t\tbert.transformer.layer.0.sa_layer_norm.weight\n",
      "False\t768\t\t\tbert.transformer.layer.0.sa_layer_norm.bias\n",
      "False\t2359296\t\t\tbert.transformer.layer.0.ffn.lin1.weight\n",
      "False\t3072\t\t\tbert.transformer.layer.0.ffn.lin1.bias\n",
      "False\t2359296\t\t\tbert.transformer.layer.0.ffn.lin2.weight\n",
      "False\t768\t\t\tbert.transformer.layer.0.ffn.lin2.bias\n",
      "False\t768\t\t\tbert.transformer.layer.0.output_layer_norm.weight\n",
      "False\t768\t\t\tbert.transformer.layer.0.output_layer_norm.bias\n",
      "False\t589824\t\t\tbert.transformer.layer.1.attention.q_lin.weight\n",
      "False\t768\t\t\tbert.transformer.layer.1.attention.q_lin.bias\n",
      "False\t589824\t\t\tbert.transformer.layer.1.attention.k_lin.weight\n",
      "False\t768\t\t\tbert.transformer.layer.1.attention.k_lin.bias\n",
      "False\t589824\t\t\tbert.transformer.layer.1.attention.v_lin.weight\n",
      "False\t768\t\t\tbert.transformer.layer.1.attention.v_lin.bias\n",
      "False\t589824\t\t\tbert.transformer.layer.1.attention.out_lin.weight\n",
      "False\t768\t\t\tbert.transformer.layer.1.attention.out_lin.bias\n",
      "False\t768\t\t\tbert.transformer.layer.1.sa_layer_norm.weight\n",
      "False\t768\t\t\tbert.transformer.layer.1.sa_layer_norm.bias\n",
      "False\t2359296\t\t\tbert.transformer.layer.1.ffn.lin1.weight\n",
      "False\t3072\t\t\tbert.transformer.layer.1.ffn.lin1.bias\n",
      "False\t2359296\t\t\tbert.transformer.layer.1.ffn.lin2.weight\n",
      "False\t768\t\t\tbert.transformer.layer.1.ffn.lin2.bias\n",
      "False\t768\t\t\tbert.transformer.layer.1.output_layer_norm.weight\n",
      "False\t768\t\t\tbert.transformer.layer.1.output_layer_norm.bias\n",
      "False\t589824\t\t\tbert.transformer.layer.2.attention.q_lin.weight\n",
      "False\t768\t\t\tbert.transformer.layer.2.attention.q_lin.bias\n",
      "False\t589824\t\t\tbert.transformer.layer.2.attention.k_lin.weight\n",
      "False\t768\t\t\tbert.transformer.layer.2.attention.k_lin.bias\n",
      "False\t589824\t\t\tbert.transformer.layer.2.attention.v_lin.weight\n",
      "False\t768\t\t\tbert.transformer.layer.2.attention.v_lin.bias\n",
      "False\t589824\t\t\tbert.transformer.layer.2.attention.out_lin.weight\n",
      "False\t768\t\t\tbert.transformer.layer.2.attention.out_lin.bias\n",
      "False\t768\t\t\tbert.transformer.layer.2.sa_layer_norm.weight\n",
      "False\t768\t\t\tbert.transformer.layer.2.sa_layer_norm.bias\n",
      "False\t2359296\t\t\tbert.transformer.layer.2.ffn.lin1.weight\n",
      "False\t3072\t\t\tbert.transformer.layer.2.ffn.lin1.bias\n",
      "False\t2359296\t\t\tbert.transformer.layer.2.ffn.lin2.weight\n",
      "False\t768\t\t\tbert.transformer.layer.2.ffn.lin2.bias\n",
      "False\t768\t\t\tbert.transformer.layer.2.output_layer_norm.weight\n",
      "False\t768\t\t\tbert.transformer.layer.2.output_layer_norm.bias\n",
      "False\t589824\t\t\tbert.transformer.layer.3.attention.q_lin.weight\n",
      "False\t768\t\t\tbert.transformer.layer.3.attention.q_lin.bias\n",
      "False\t589824\t\t\tbert.transformer.layer.3.attention.k_lin.weight\n",
      "False\t768\t\t\tbert.transformer.layer.3.attention.k_lin.bias\n",
      "False\t589824\t\t\tbert.transformer.layer.3.attention.v_lin.weight\n",
      "False\t768\t\t\tbert.transformer.layer.3.attention.v_lin.bias\n",
      "False\t589824\t\t\tbert.transformer.layer.3.attention.out_lin.weight\n",
      "False\t768\t\t\tbert.transformer.layer.3.attention.out_lin.bias\n",
      "False\t768\t\t\tbert.transformer.layer.3.sa_layer_norm.weight\n",
      "False\t768\t\t\tbert.transformer.layer.3.sa_layer_norm.bias\n",
      "False\t2359296\t\t\tbert.transformer.layer.3.ffn.lin1.weight\n",
      "False\t3072\t\t\tbert.transformer.layer.3.ffn.lin1.bias\n",
      "False\t2359296\t\t\tbert.transformer.layer.3.ffn.lin2.weight\n",
      "False\t768\t\t\tbert.transformer.layer.3.ffn.lin2.bias\n",
      "False\t768\t\t\tbert.transformer.layer.3.output_layer_norm.weight\n",
      "False\t768\t\t\tbert.transformer.layer.3.output_layer_norm.bias\n",
      "False\t589824\t\t\tbert.transformer.layer.4.attention.q_lin.weight\n",
      "False\t768\t\t\tbert.transformer.layer.4.attention.q_lin.bias\n",
      "False\t589824\t\t\tbert.transformer.layer.4.attention.k_lin.weight\n",
      "False\t768\t\t\tbert.transformer.layer.4.attention.k_lin.bias\n",
      "False\t589824\t\t\tbert.transformer.layer.4.attention.v_lin.weight\n",
      "False\t768\t\t\tbert.transformer.layer.4.attention.v_lin.bias\n",
      "False\t589824\t\t\tbert.transformer.layer.4.attention.out_lin.weight\n",
      "False\t768\t\t\tbert.transformer.layer.4.attention.out_lin.bias\n",
      "False\t768\t\t\tbert.transformer.layer.4.sa_layer_norm.weight\n",
      "False\t768\t\t\tbert.transformer.layer.4.sa_layer_norm.bias\n",
      "False\t2359296\t\t\tbert.transformer.layer.4.ffn.lin1.weight\n",
      "False\t3072\t\t\tbert.transformer.layer.4.ffn.lin1.bias\n",
      "False\t2359296\t\t\tbert.transformer.layer.4.ffn.lin2.weight\n",
      "False\t768\t\t\tbert.transformer.layer.4.ffn.lin2.bias\n",
      "False\t768\t\t\tbert.transformer.layer.4.output_layer_norm.weight\n",
      "False\t768\t\t\tbert.transformer.layer.4.output_layer_norm.bias\n",
      "True\t589824\t\t\tbert.transformer.layer.5.attention.q_lin.weight\n",
      "True\t768\t\t\tbert.transformer.layer.5.attention.q_lin.bias\n",
      "True\t589824\t\t\tbert.transformer.layer.5.attention.k_lin.weight\n",
      "True\t768\t\t\tbert.transformer.layer.5.attention.k_lin.bias\n",
      "True\t589824\t\t\tbert.transformer.layer.5.attention.v_lin.weight\n",
      "True\t768\t\t\tbert.transformer.layer.5.attention.v_lin.bias\n",
      "True\t589824\t\t\tbert.transformer.layer.5.attention.out_lin.weight\n",
      "True\t768\t\t\tbert.transformer.layer.5.attention.out_lin.bias\n",
      "True\t768\t\t\tbert.transformer.layer.5.sa_layer_norm.weight\n",
      "True\t768\t\t\tbert.transformer.layer.5.sa_layer_norm.bias\n",
      "True\t2359296\t\t\tbert.transformer.layer.5.ffn.lin1.weight\n",
      "True\t3072\t\t\tbert.transformer.layer.5.ffn.lin1.bias\n",
      "True\t2359296\t\t\tbert.transformer.layer.5.ffn.lin2.weight\n",
      "True\t768\t\t\tbert.transformer.layer.5.ffn.lin2.bias\n",
      "True\t768\t\t\tbert.transformer.layer.5.output_layer_norm.weight\n",
      "True\t768\t\t\tbert.transformer.layer.5.output_layer_norm.bias\n",
      "True\t1536\t\t\tcls.weight\n",
      "True\t2\t\t\tcls.bias\n",
      "The network has 66364418 parameters, out of which 7089410 (10.7%) are trainable.\n"
     ]
    }
   ],
   "source": [
    "model.freeze_until_layer(5)\n",
    "model.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc478e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "  # Pregatim o modalitate de stocare a datelor pentru evaluare\n",
    "  eval_outputs = []\n",
    "  true_labels = []\n",
    "  # Trecem modelul in modul train\n",
    "  model.eval()\n",
    "\n",
    "  ########### Evaluation Loop #############\n",
    "  with torch.no_grad():\n",
    "      for batch in tqdm(test_loader, total=len(test_loader)):\n",
    "          outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "\n",
    "          outputs = outputs.cpu() ## copy-ing the outputs from CUDA to CPU\n",
    "          outputs = np.argmax(outputs, axis=1)\n",
    "\n",
    "          eval_outputs += outputs.tolist()\n",
    "          true_labels += batch['labels'].tolist()\n",
    "\n",
    "  #acc = metrics.accuracy_score(true_labels, eval_outputs)\n",
    "  f1 = metrics.f1_score(true_labels, eval_outputs)\n",
    "  print(\"F1: {}\".format(f1) , end =\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8da9ce98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                  | 0/2500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running epoch 0 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 2500/2500 [1:12:41<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with train loss: 0.47198699218034745 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 625/625 [14:08<00:00,  1.36s/it]\n",
      "  0%|                                                  | 0/2500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.859075535512965 Running epoch 1 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 2500/2500 [1:10:32<00:00,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with train loss: 0.438261607503891 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 625/625 [14:10<00:00,  1.36s/it]\n",
      "  0%|                                                  | 0/2500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.8652489367025269 Running epoch 2 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 2500/2500 [1:10:33<00:00,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with train loss: 0.42452582869529726 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 625/625 [14:09<00:00,  1.36s/it]\n",
      "  0%|                                                  | 0/2500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.864169927333706 Running epoch 3 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 2500/2500 [1:10:43<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with train loss: 0.4136776436805725 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 625/625 [14:09<00:00,  1.36s/it]\n",
      "  0%|                                                  | 0/2500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.8672026050676707 Running epoch 4 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 2500/2500 [1:10:43<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with train loss: 0.401535007750988 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 625/625 [14:08<00:00,  1.36s/it]\n",
      "  0%|                                                  | 0/2500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.8736722493947926 Running epoch 5 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 2500/2500 [1:10:47<00:00,  1.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with train loss: 0.3913104826927185 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 625/625 [14:11<00:00,  1.36s/it]\n",
      "  0%|                                                  | 0/2500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.8619549493697044 Running epoch 6 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 2500/2500 [1:11:11<00:00,  1.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model with train loss: 0.3827972535967827 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 625/625 [14:21<00:00,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.86865598027127 Early Stopping!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 5e-5 # Rata de invatare\n",
    "NR_EPOCHS = 8 # Numarul de epoci\n",
    "BATCH_SIZE = 32 # Numarul de samples dintr-un batch\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Pregatim o modalitate de loggare a informatiilor din timpul antrenarii\n",
    "log_info = []\n",
    "\n",
    "# Pregatim DataLoader-ul pentru antrenare\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Pregatim DataLoader-ul pentru validare\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=32, shuffle=False)\n",
    "\n",
    "# Trecem modelul in modul train\n",
    "model.train() \n",
    "\n",
    "\n",
    "########### Training Loop #############\n",
    "\n",
    "\n",
    "min_epoch_loss = np.Inf\n",
    "last_epoch_loss = np.Inf\n",
    "# pentru fiecare epoca (1 epoca = o iteratie peste intregul set de date)\n",
    "for epoch in range(NR_EPOCHS):\n",
    "    print('Running epoch {}'.format(epoch), end =\" \")\n",
    "\n",
    "    epoch_losses = []\n",
    "    # pentru fiecare batch de BATCH_SIZE exemple din setul de date    \n",
    "    for i, batch in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "\n",
    "        # anulam gradientii deja acumulati la nivelul retelei neuronale\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # FORWARD PASS: trecem inputurile prin retea\n",
    "        outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "\n",
    "        # Calculam LOSSul dintre etichetele prezise si cele reale\n",
    "        loss = criterion(outputs, batch['labels'])\n",
    "\n",
    "        # BACKPRPAGATION: calculam gradientii propagand LOSSul in retea\n",
    "        loss.backward()\n",
    "\n",
    "        # Utilizam optimizorul pentru a modifica parametrii retelei in functie de gradientii acumulati\n",
    "        optimizer.step()\n",
    "\n",
    "        # Salvam informatii despre antrenare (in cazul nostru, salvam valoarea LOSS)\n",
    "        epoch_losses.append(loss.item()) \n",
    "    \n",
    "    this_epoch_loss = np.mean(epoch_losses)\n",
    "    log_info.append((epoch, this_epoch_loss))\n",
    "    \n",
    "    if this_epoch_loss <= min_epoch_loss:\n",
    "      min_epoch_loss = this_epoch_loss\n",
    "      print(\"Saving model with train loss: {}\".format(this_epoch_loss), end =\" \")\n",
    "      torch.save(model, \"classfication_model.pt\")\n",
    "\n",
    "    evaluate(model)\n",
    "    \n",
    "    if last_epoch_loss - this_epoch_loss < 0.01:\n",
    "      print(\"Early Stopping!\")\n",
    "      break\n",
    "\n",
    "    last_epoch_loss = this_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2eefc2e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = [x for x, loss in log_info]\n",
    "Y = [loss for x, loss in log_info]\n",
    "plt.plot(X,Y)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"LOSS\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2531efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 625/625 [14:20<00:00,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.86865598027127 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9ebaa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
