{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "222e4d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import AutoTokenizer, DistilBertModel\n",
    "from transformers import AdamW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4effb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce GTX 1050 Ti\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "  for i in range(torch.cuda.device_count()):\n",
    "    print(torch.cuda.get_device_name(i))\n",
    "else:\n",
    "  print(\"You are running on CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef0d75c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Datasets/Cuvinte-Eliminate/train-punct-stop-1000.csv\")\n",
    "df=df.dropna()\n",
    "df=df.sample(n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f623596e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>332584</th>\n",
       "      <td>2</td>\n",
       "      <td>great inspiring book authors show wonderfully ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2347926</th>\n",
       "      <td>0</td>\n",
       "      <td>sorry pretty messed release theatrical edition...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723728</th>\n",
       "      <td>0</td>\n",
       "      <td>big blues fan since 8 yrs old excited first he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857228</th>\n",
       "      <td>2</td>\n",
       "      <td>straight forward install use no surprises tigh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116169</th>\n",
       "      <td>2</td>\n",
       "      <td>hands no better live act rock today cannot get...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment                                               text\n",
       "332584           2  great inspiring book authors show wonderfully ...\n",
       "2347926          0  sorry pretty messed release theatrical edition...\n",
       "1723728          0  big blues fan since 8 yrs old excited first he...\n",
       "857228           2  straight forward install use no surprises tigh...\n",
       "116169           2  hands no better live act rock today cannot get..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc3a8b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df['sentiment'].replace(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53c66935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>332584</th>\n",
       "      <td>1</td>\n",
       "      <td>great inspiring book authors show wonderfully ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2347926</th>\n",
       "      <td>0</td>\n",
       "      <td>sorry pretty messed release theatrical edition...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723728</th>\n",
       "      <td>0</td>\n",
       "      <td>big blues fan since 8 yrs old excited first he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857228</th>\n",
       "      <td>1</td>\n",
       "      <td>straight forward install use no surprises tigh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116169</th>\n",
       "      <td>1</td>\n",
       "      <td>hands no better live act rock today cannot get...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment                                               text\n",
       "332584           1  great inspiring book authors show wonderfully ...\n",
       "2347926          0  sorry pretty messed release theatrical edition...\n",
       "1723728          0  big blues fan since 8 yrs old excited first he...\n",
       "857228           1  straight forward install use no surprises tigh...\n",
       "116169           1  hands no better live act rock today cannot get..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38d11fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "def tokenize_function(example):\n",
    "    tok = tokenizer(example, padding=\"max_length\", truncation=True)\n",
    "    return tok['input_ids'], tok['attention_mask']\n",
    "    \n",
    "df['input_ids'], df['attention_mask'] = zip(*df['text'].map(tokenize_function))\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f509f2b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>332584</th>\n",
       "      <td>1</td>\n",
       "      <td>great inspiring book authors show wonderfully ...</td>\n",
       "      <td>[101, 2307, 18988, 2338, 6048, 2265, 6919, 213...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2347926</th>\n",
       "      <td>0</td>\n",
       "      <td>sorry pretty messed release theatrical edition...</td>\n",
       "      <td>[101, 3374, 3492, 18358, 2713, 8900, 3179, 202...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1723728</th>\n",
       "      <td>0</td>\n",
       "      <td>big blues fan since 8 yrs old excited first he...</td>\n",
       "      <td>[101, 2502, 5132, 5470, 2144, 1022, 1061, 2869...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857228</th>\n",
       "      <td>1</td>\n",
       "      <td>straight forward install use no surprises tigh...</td>\n",
       "      <td>[101, 3442, 2830, 16500, 2224, 2053, 20096, 43...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116169</th>\n",
       "      <td>1</td>\n",
       "      <td>hands no better live act rock today cannot get...</td>\n",
       "      <td>[101, 2398, 2053, 2488, 2444, 2552, 2600, 2651...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment                                               text  \\\n",
       "332584           1  great inspiring book authors show wonderfully ...   \n",
       "2347926          0  sorry pretty messed release theatrical edition...   \n",
       "1723728          0  big blues fan since 8 yrs old excited first he...   \n",
       "857228           1  straight forward install use no surprises tigh...   \n",
       "116169           1  hands no better live act rock today cannot get...   \n",
       "\n",
       "                                                 input_ids  \\\n",
       "332584   [101, 2307, 18988, 2338, 6048, 2265, 6919, 213...   \n",
       "2347926  [101, 3374, 3492, 18358, 2713, 8900, 3179, 202...   \n",
       "1723728  [101, 2502, 5132, 5470, 2144, 1022, 1061, 2869...   \n",
       "857228   [101, 3442, 2830, 16500, 2224, 2053, 20096, 43...   \n",
       "116169   [101, 2398, 2053, 2488, 2444, 2552, 2600, 2651...   \n",
       "\n",
       "                                            attention_mask  \n",
       "332584   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2347926  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1723728  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "857228   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "116169   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b87b37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.df.iloc[idx]\n",
    "        return {\n",
    "            'input_ids' : torch.tensor(item['input_ids']).to(device),\n",
    "            'attention_mask' : torch.tensor(item['attention_mask']).to(device),\n",
    "            'labels' : torch.tensor(item['sentiment']).to(device)\n",
    "        }\n",
    "\n",
    "train_set = Dataset(df_train)\n",
    "test_set = Dataset(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd49b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, out_feat=2):\n",
    "        super().__init__()\n",
    "        self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.cls = nn.Linear(768, out_feat)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    " \n",
    "        pooled_output = outputs.last_hidden_state[:,0,:] #[batch_dim, token_dim, ...] [CLS]\n",
    "        logits = self.cls(pooled_output)\n",
    "        return F.softmax(logits, dim=1)\n",
    "\n",
    "    def freeze_until_layer(self, n):\n",
    "      for name, param in self.named_parameters():\n",
    "        if str(n) in name:\n",
    "          break\n",
    "\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    def print_layers(self):\n",
    "      total_nr_w = 0\n",
    "      trainable_nr_w = 0\n",
    "      for name, param in self.named_parameters():\n",
    "        nr_w = np.prod(param.size())\n",
    "        total_nr_w += nr_w\n",
    "        if param.requires_grad:\n",
    "          trainable_nr_w += nr_w\n",
    "        print('{}\\t{}\\t\\t\\t{}'.format(param.requires_grad, nr_w ,name))\n",
    "      print('The network has {} parameters, out of which {} ({:.1f}%) are trainable.'.format(total_nr_w, trainable_nr_w, trainable_nr_w / total_nr_w * 100))\n",
    "\n",
    "model = Classifier(2).to(device) # generez o instanță a modelului\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e47f9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.freeze_until_layer(5)\n",
    "model.print_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc478e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "  # Pregatim o modalitate de stocare a datelor pentru evaluare\n",
    "  eval_outputs = []\n",
    "  true_labels = []\n",
    "  # Trecem modelul in modul train\n",
    "  model.eval()\n",
    "\n",
    "  ########### Evaluation Loop #############\n",
    "  with torch.no_grad():\n",
    "      for batch in tqdm(test_loader, total=len(test_loader)):\n",
    "          outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "\n",
    "          outputs = outputs.cpu() ## copy-ing the outputs from CUDA to CPU\n",
    "          outputs = np.argmax(outputs, axis=1)\n",
    "\n",
    "          eval_outputs += outputs.tolist()\n",
    "          true_labels += batch['labels'].tolist()\n",
    "\n",
    "  #acc = metrics.accuracy_score(true_labels, eval_outputs)\n",
    "  f1 = metrics.f1_score(true_labels, eval_outputs)\n",
    "  print(\"F1: {}\".format(f1) , end =\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da9ce98",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 5e-5 # Rata de invatare\n",
    "NR_EPOCHS = 8 # Numarul de epoci\n",
    "BATCH_SIZE = 32 # Numarul de samples dintr-un batch\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Pregatim o modalitate de loggare a informatiilor din timpul antrenarii\n",
    "log_info = []\n",
    "\n",
    "# Pregatim DataLoader-ul pentru antrenare\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Pregatim DataLoader-ul pentru validare\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=32, shuffle=False)\n",
    "\n",
    "# Trecem modelul in modul train\n",
    "model.train() \n",
    "\n",
    "\n",
    "########### Training Loop #############\n",
    "\n",
    "\n",
    "min_epoch_loss = np.Inf\n",
    "last_epoch_loss = np.Inf\n",
    "# pentru fiecare epoca (1 epoca = o iteratie peste intregul set de date)\n",
    "for epoch in range(NR_EPOCHS):\n",
    "    print('Running epoch {}'.format(epoch), end =\" \")\n",
    "\n",
    "    epoch_losses = []\n",
    "    # pentru fiecare batch de BATCH_SIZE exemple din setul de date    \n",
    "    for i, batch in tqdm(enumerate(train_loader), total=len(train_loader)):\n",
    "\n",
    "        # anulam gradientii deja acumulati la nivelul retelei neuronale\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # FORWARD PASS: trecem inputurile prin retea\n",
    "        outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "\n",
    "        # Calculam LOSSul dintre etichetele prezise si cele reale\n",
    "        loss = criterion(outputs, batch['labels'])\n",
    "\n",
    "        # BACKPRPAGATION: calculam gradientii propagand LOSSul in retea\n",
    "        loss.backward()\n",
    "\n",
    "        # Utilizam optimizorul pentru a modifica parametrii retelei in functie de gradientii acumulati\n",
    "        optimizer.step()\n",
    "\n",
    "        # Salvam informatii despre antrenare (in cazul nostru, salvam valoarea LOSS)\n",
    "        epoch_losses.append(loss.item()) \n",
    "    \n",
    "    this_epoch_loss = np.mean(epoch_losses)\n",
    "    log_info.append((epoch, this_epoch_loss))\n",
    "    \n",
    "    if this_epoch_loss <= min_epoch_loss:\n",
    "      min_epoch_loss = this_epoch_loss\n",
    "      print(\"Saving model with train loss: {}\".format(this_epoch_loss), end =\" \")\n",
    "      torch.save(model, \"classfication_model.pt\")\n",
    "\n",
    "    evaluate(model)\n",
    "    \n",
    "    if last_epoch_loss - this_epoch_loss < 0.01:\n",
    "      print(\"Early Stopping!\")\n",
    "      break\n",
    "\n",
    "    last_epoch_loss = this_epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eefc2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [x for x, loss in log_info]\n",
    "Y = [loss for x, loss in log_info]\n",
    "plt.plot(X,Y)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"LOSS\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2531efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f48915f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
