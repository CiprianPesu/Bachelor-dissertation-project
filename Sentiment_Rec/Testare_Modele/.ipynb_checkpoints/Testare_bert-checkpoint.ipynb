{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47a6b618",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "import glob\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.stem import PorterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, DistilBertModel\n",
    "from transformers import AdamW\n",
    "import transformers\n",
    "from transformers import DistilBertTokenizer\n",
    "from transformers import TFDistilBertForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3112d87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "porter=PorterStemmer()\n",
    "stopwords=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y']\n",
    "import pickle\n",
    "with open('tokenizer-200.pickle', 'rb') as handle:\n",
    "    words = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0c60aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "words_list=json.loads(words.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "90cc4280",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=json.loads(words_list[\"config\"][\"word_counts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a7541f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=list(y.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8064d544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apron',\n",
       " 'show',\n",
       " 'plu',\n",
       " 'fade',\n",
       " 'quickli',\n",
       " 'expos',\n",
       " 'big',\n",
       " 'deal',\n",
       " 'within',\n",
       " 'week',\n",
       " 'use',\n",
       " 'around',\n",
       " 'fabric',\n",
       " 'stitch',\n",
       " 'attach',\n",
       " 'one',\n",
       " 'pocket',\n",
       " 'toward',\n",
       " 'center',\n",
       " 'start',\n",
       " 'rip',\n",
       " 'not',\n",
       " 'stay',\n",
       " 'tear',\n",
       " 'loos',\n",
       " 'appear',\n",
       " 'snag',\n",
       " 'someth',\n",
       " 'move',\n",
       " 'shop',\n",
       " 'expect',\n",
       " 'otherwis',\n",
       " 'ok',\n",
       " 'price',\n",
       " 'no',\n",
       " 'dock',\n",
       " 'web',\n",
       " 'cam',\n",
       " 'featur',\n",
       " 'ever',\n",
       " 'buy',\n",
       " 'thisth',\n",
       " 'zoom',\n",
       " 'stuck',\n",
       " 'never',\n",
       " 'set',\n",
       " 'foot',\n",
       " 'danc',\n",
       " 'studio',\n",
       " 'might',\n",
       " 'enjoy',\n",
       " 'video',\n",
       " 'anyon',\n",
       " '2',\n",
       " 'lesson',\n",
       " 'would',\n",
       " 'feel',\n",
       " 'wast',\n",
       " 'money',\n",
       " 'among',\n",
       " 'us',\n",
       " 'hasnt',\n",
       " 'notic',\n",
       " 'centuri',\n",
       " 'human',\n",
       " 'seem',\n",
       " 'becom',\n",
       " 'insul',\n",
       " 'selfinvolv',\n",
       " 'less',\n",
       " 'mind',\n",
       " 'other',\n",
       " 'hope',\n",
       " 'lynn',\n",
       " 'truss',\n",
       " 'offer',\n",
       " 'humor',\n",
       " 'observ',\n",
       " 'subject',\n",
       " 'along',\n",
       " 'tongueincheek',\n",
       " 'way',\n",
       " 'possibl',\n",
       " 'bit',\n",
       " 'insight',\n",
       " 'thought',\n",
       " 'thing',\n",
       " 'lot',\n",
       " 'whine',\n",
       " 'person',\n",
       " 'petti',\n",
       " 'annoy',\n",
       " 'credit',\n",
       " 'write',\n",
       " 'style',\n",
       " 'extrem',\n",
       " 'witti',\n",
       " 'find',\n",
       " 'laugh',\n",
       " 'loud',\n",
       " 'read',\n",
       " 'passag',\n",
       " 'overal',\n",
       " 'felt',\n",
       " 'like',\n",
       " 'listen',\n",
       " 'gripe',\n",
       " 'friend',\n",
       " 'afternoon',\n",
       " 'hand',\n",
       " 'bring',\n",
       " 'forefront',\n",
       " 'imperson',\n",
       " 'weve',\n",
       " 'there',\n",
       " 'much',\n",
       " 'point',\n",
       " 'rais',\n",
       " 'awar',\n",
       " 'alreadi',\n",
       " 'look',\n",
       " 'substanc',\n",
       " 'ms',\n",
       " 'told',\n",
       " 'talk',\n",
       " 'saw',\n",
       " 'come',\n",
       " 'compat',\n",
       " 'basic',\n",
       " 'command',\n",
       " 'control',\n",
       " 'tv',\n",
       " 'volum',\n",
       " 'channel',\n",
       " 'chang',\n",
       " 'muchit',\n",
       " 'cheap',\n",
       " 'good',\n",
       " 'product',\n",
       " 'issu',\n",
       " 'indic',\n",
       " 'remot',\n",
       " 'mode',\n",
       " 'record',\n",
       " 'press',\n",
       " 'arrow',\n",
       " 'doesnt',\n",
       " 'light',\n",
       " 'know',\n",
       " 'absolut',\n",
       " 'rivet',\n",
       " 'mani',\n",
       " 'fascin',\n",
       " 'tale',\n",
       " 'young',\n",
       " 'american',\n",
       " 'endur',\n",
       " 'time',\n",
       " 'great',\n",
       " 'struggl',\n",
       " 'reread',\n",
       " 'outstand',\n",
       " 'work',\n",
       " 'lost',\n",
       " 'mr',\n",
       " 'uri',\n",
       " 'compliment',\n",
       " 'excel',\n",
       " 'gift',\n",
       " 'reader',\n",
       " 'bought',\n",
       " 'son',\n",
       " '4th',\n",
       " 'golf',\n",
       " 'go',\n",
       " 'first',\n",
       " 'real',\n",
       " 'club',\n",
       " 'item',\n",
       " 'put',\n",
       " 'togeth',\n",
       " 'batteri',\n",
       " 'fulli',\n",
       " 'charg',\n",
       " 'excit',\n",
       " 'hop',\n",
       " 'right',\n",
       " 'went',\n",
       " 'nowher',\n",
       " 'pice',\n",
       " 'junk',\n",
       " 'weigh',\n",
       " 'level',\n",
       " 'lawn',\n",
       " 'grass',\n",
       " 'cut',\n",
       " 'previou',\n",
       " 'day',\n",
       " '1',\n",
       " 'back',\n",
       " 'wheel',\n",
       " 'pull',\n",
       " 'simpli',\n",
       " 'power',\n",
       " 'perhap',\n",
       " 'better',\n",
       " 'driveway',\n",
       " 'alot',\n",
       " 'ride',\n",
       " 'deer',\n",
       " 'tractor',\n",
       " 'he',\n",
       " 'year',\n",
       " 'run',\n",
       " 'recommend',\n",
       " 'unless',\n",
       " 'strictli',\n",
       " 'cours',\n",
       " 'made',\n",
       " 'concret',\n",
       " 'though',\n",
       " 'cd',\n",
       " 'pretens',\n",
       " 'sonata',\n",
       " 'anywher',\n",
       " 'near',\n",
       " 'everi',\n",
       " 'song',\n",
       " 'sound',\n",
       " 'pretti',\n",
       " 'titl',\n",
       " 'track',\n",
       " 'usual',\n",
       " 'choru',\n",
       " 'sung',\n",
       " 'album',\n",
       " 'repetit',\n",
       " 'almost',\n",
       " 'varieti',\n",
       " 'differ',\n",
       " 'last',\n",
       " 'instrument',\n",
       " 'dont',\n",
       " 'realli',\n",
       " 'hardcor',\n",
       " 'metal',\n",
       " 'fan',\n",
       " 'happi',\n",
       " 'list',\n",
       " 'book',\n",
       " 'order',\n",
       " 'condit',\n",
       " 'email',\n",
       " 'bob',\n",
       " 'got',\n",
       " 'follow',\n",
       " 'could',\n",
       " 'understood',\n",
       " 'provid',\n",
       " 'proper',\n",
       " 'custom',\n",
       " 'servic',\n",
       " 'origin',\n",
       " 'convinc',\n",
       " 'consid',\n",
       " 'stand',\n",
       " 'behind',\n",
       " 'didnt',\n",
       " 'death',\n",
       " '20',\n",
       " 'cool',\n",
       " 'think',\n",
       " 'breed',\n",
       " 'best',\n",
       " 'introduc',\n",
       " 'amaz',\n",
       " 'take',\n",
       " 'florida',\n",
       " 'master',\n",
       " 'naturalist',\n",
       " 'class',\n",
       " 'extens',\n",
       " 'univers',\n",
       " 'want',\n",
       " 'understand',\n",
       " 'complex',\n",
       " 'divis',\n",
       " 'explan',\n",
       " 'well',\n",
       " 'written',\n",
       " 'approach',\n",
       " 'studi',\n",
       " 'begin',\n",
       " 'wasnt',\n",
       " 'worst',\n",
       " 'movi',\n",
       " 'seeni',\n",
       " 'lame',\n",
       " 'action',\n",
       " 'itin',\n",
       " 'opinion',\n",
       " 'also',\n",
       " 'miscast',\n",
       " 'chri',\n",
       " 'klein',\n",
       " 'herei',\n",
       " 'ace',\n",
       " 'charact',\n",
       " 'belong',\n",
       " 'michael',\n",
       " 'clark',\n",
       " 'duncan',\n",
       " 'play',\n",
       " 'role',\n",
       " 'alway',\n",
       " 'doubt',\n",
       " 'talent',\n",
       " 'wonder',\n",
       " 'bodi',\n",
       " 'didth',\n",
       " 'decent',\n",
       " 'moon',\n",
       " 'entir',\n",
       " 'legend',\n",
       " '175',\n",
       " 'instruct',\n",
       " 'pump',\n",
       " 'cleans',\n",
       " 'mitchel',\n",
       " 'keep',\n",
       " 'wen',\n",
       " 'algebra',\n",
       " 'fundament',\n",
       " 'far',\n",
       " 'cover',\n",
       " 'exampl',\n",
       " 'problem',\n",
       " 'pay',\n",
       " 'brand',\n",
       " 'new',\n",
       " 'shape',\n",
       " 'descript',\n",
       " 'care',\n",
       " 'make',\n",
       " 'sure',\n",
       " 'get',\n",
       " 'badli',\n",
       " 'view',\n",
       " 'life',\n",
       " 'theori',\n",
       " 'percept',\n",
       " 'quit',\n",
       " 'explain',\n",
       " 'meif',\n",
       " 'your',\n",
       " 'open',\n",
       " 'everyth',\n",
       " 'wrong',\n",
       " 'film',\n",
       " 'believ',\n",
       " 'see',\n",
       " 'wont',\n",
       " 'idea',\n",
       " 'present',\n",
       " 'belief',\n",
       " 'mix',\n",
       " 'grand',\n",
       " 'absurd',\n",
       " 'hold',\n",
       " 'merit',\n",
       " 'advantag',\n",
       " 'say',\n",
       " 'quantum',\n",
       " 'physic',\n",
       " 'that',\n",
       " 'bs',\n",
       " 'secondli',\n",
       " 'ive',\n",
       " 'knowledg',\n",
       " 'base',\n",
       " 'im',\n",
       " 'lyle',\n",
       " 'behavior',\n",
       " 'turn',\n",
       " 'storylin',\n",
       " 'howev',\n",
       " 'pregnant',\n",
       " 'father',\n",
       " '15',\n",
       " 'atroci',\n",
       " 'dread',\n",
       " 'circumst',\n",
       " 'maggi',\n",
       " 'protect',\n",
       " 'final',\n",
       " 'reveal',\n",
       " 'truth',\n",
       " 'endi',\n",
       " 'romanc',\n",
       " 'emot',\n",
       " 'connect',\n",
       " 'tragic',\n",
       " 'episod',\n",
       " 'world',\n",
       " 'histori',\n",
       " 'difficult',\n",
       " 'watch',\n",
       " 'grandfath',\n",
       " 'fell',\n",
       " 'german',\n",
       " 'special',\n",
       " 'interest',\n",
       " 'even',\n",
       " 'moor',\n",
       " 'perfect',\n",
       " 'palin',\n",
       " 'scene',\n",
       " 'campaign',\n",
       " 'happen',\n",
       " 'desir',\n",
       " 'win',\n",
       " 'place',\n",
       " 'countri',\n",
       " 'christma',\n",
       " 'sister',\n",
       " 'comput',\n",
       " 'yeah',\n",
       " 'socal',\n",
       " 'hip',\n",
       " 'peopl',\n",
       " 'squeaki',\n",
       " 'clean',\n",
       " 'imagin',\n",
       " 'mean',\n",
       " 'amazon',\n",
       " 'review',\n",
       " 'miss',\n",
       " 'joke',\n",
       " 'materi',\n",
       " 'chose',\n",
       " 'richard',\n",
       " 'chees',\n",
       " 'choos',\n",
       " 'whew',\n",
       " 'late',\n",
       " 'daddi',\n",
       " 'beg',\n",
       " 'ozzi',\n",
       " 'call',\n",
       " 'next',\n",
       " 'door',\n",
       " 'neighbor',\n",
       " 'presenc',\n",
       " 'osbourn',\n",
       " 'famili',\n",
       " 'kind',\n",
       " 'stuff',\n",
       " 'tonight',\n",
       " 'lennon',\n",
       " 'sometim',\n",
       " 'artist',\n",
       " 'need',\n",
       " 'selfright',\n",
       " 'bubbl',\n",
       " 'pop',\n",
       " 'ask',\n",
       " '2007',\n",
       " 'bad',\n",
       " 'pain',\n",
       " 'tri',\n",
       " 'finish',\n",
       " 'author',\n",
       " 'heard',\n",
       " 'needless',\n",
       " 'probabl',\n",
       " 'favorit',\n",
       " 'disappoint',\n",
       " 'tire',\n",
       " 'kingdom',\n",
       " '04',\n",
       " '07',\n",
       " 'plop',\n",
       " '85',\n",
       " 'month',\n",
       " '3',\n",
       " 'replac',\n",
       " '12',\n",
       " 'retail',\n",
       " 'carri',\n",
       " 'buyer',\n",
       " 'bewar',\n",
       " 'stori',\n",
       " 'suspens',\n",
       " 'done',\n",
       " 'without',\n",
       " 'swear',\n",
       " 'scari',\n",
       " 'intens',\n",
       " 'arent',\n",
       " 'kristen',\n",
       " 'sever',\n",
       " 'blue',\n",
       " 'tooth',\n",
       " 'ear',\n",
       " 'piec',\n",
       " 'luck',\n",
       " 'whatsoev',\n",
       " 'total',\n",
       " 'unreli',\n",
       " 'aw',\n",
       " 'everyon',\n",
       " 'water',\n",
       " 'decid',\n",
       " 'impress',\n",
       " 'actual',\n",
       " 'hear',\n",
       " 'clear',\n",
       " 'certainli',\n",
       " 'abl',\n",
       " 'drive',\n",
       " 'handsfre',\n",
       " 'rememb',\n",
       " 'technolog',\n",
       " 'still',\n",
       " 'leav',\n",
       " 'quirk',\n",
       " 'dual',\n",
       " 'monitor',\n",
       " 'tablet',\n",
       " 'geniu',\n",
       " 'technic',\n",
       " 'support',\n",
       " 'said',\n",
       " 'sorri',\n",
       " 'none',\n",
       " 'licens',\n",
       " 'yuck',\n",
       " 'invest',\n",
       " '24',\n",
       " 'graphic',\n",
       " 'useless',\n",
       " 'solut',\n",
       " 'pleas',\n",
       " 'post',\n",
       " 'comment',\n",
       " 'thank',\n",
       " 'farberwar',\n",
       " 'pressur',\n",
       " 'cooker',\n",
       " 'hassl',\n",
       " 'hard',\n",
       " 'tell',\n",
       " 'couldnt',\n",
       " 'exactli',\n",
       " 'long',\n",
       " 'cook',\n",
       " 'food',\n",
       " 'minut',\n",
       " 'end',\n",
       " 'mush',\n",
       " 'took',\n",
       " 'forev',\n",
       " 'pot',\n",
       " 'ran',\n",
       " 'cold',\n",
       " 'wouldnt',\n",
       " 'wait',\n",
       " 'oven',\n",
       " 'instead',\n",
       " 'faster',\n",
       " 'presto',\n",
       " 'love',\n",
       " 'steamer',\n",
       " 'basket',\n",
       " 'regular',\n",
       " 'backpack',\n",
       " 'canon',\n",
       " 'mk',\n",
       " 'ii',\n",
       " 'len',\n",
       " '100',\n",
       " 'macro',\n",
       " 'flash',\n",
       " 'equip',\n",
       " 'incident',\n",
       " 'cf',\n",
       " 'card',\n",
       " 'extra',\n",
       " 'cloth',\n",
       " 'cleaner',\n",
       " 'etc',\n",
       " 'risk',\n",
       " 'scratch',\n",
       " 'bump',\n",
       " 'broken',\n",
       " 'mingl',\n",
       " 'camera',\n",
       " 'tote',\n",
       " 'lens',\n",
       " 'apart',\n",
       " 'plenti',\n",
       " 'storag',\n",
       " 'space',\n",
       " 'comfort',\n",
       " 'pack',\n",
       " 'nice',\n",
       " 'especi',\n",
       " 'front',\n",
       " 'option',\n",
       " 'strap',\n",
       " 'clive',\n",
       " 'cussler',\n",
       " 'render',\n",
       " 'date',\n",
       " 'includ',\n",
       " 'tomb',\n",
       " 'advertis',\n",
       " 'cant',\n",
       " 'blame',\n",
       " 'stupid',\n",
       " 'print',\n",
       " 'came',\n",
       " 'littl',\n",
       " 'guy',\n",
       " 'favor',\n",
       " 'public',\n",
       " 'lead',\n",
       " 'old',\n",
       " 'prim',\n",
       " 'rose',\n",
       " 'path',\n",
       " 'gee',\n",
       " 'aint',\n",
       " 'direct',\n",
       " 'pour',\n",
       " 'bottom',\n",
       " 'manual',\n",
       " 'defect',\n",
       " 'return',\n",
       " 'merced',\n",
       " 'lackey',\n",
       " 'she',\n",
       " 'seri',\n",
       " 'continu',\n",
       " 'third',\n",
       " 'creativ',\n",
       " 'fire',\n",
       " 'conflict',\n",
       " 'develop',\n",
       " 'danger',\n",
       " 'choic',\n",
       " 'free',\n",
       " 'bard',\n",
       " 'welcom',\n",
       " 'magic',\n",
       " 'earlier',\n",
       " 'adventur',\n",
       " 'caveat',\n",
       " 'honest',\n",
       " 'youll',\n",
       " 'top',\n",
       " 'eat',\n",
       " 'maintain',\n",
       " 'health',\n",
       " 'detail',\n",
       " 'inform',\n",
       " 'specif',\n",
       " 'categori',\n",
       " 'text',\n",
       " 'pepper',\n",
       " 'tip',\n",
       " 'suggest',\n",
       " 'amount',\n",
       " 'consumpt',\n",
       " 'potenti',\n",
       " 'highli',\n",
       " 'someon',\n",
       " 'refin',\n",
       " 'menu',\n",
       " 'increas',\n",
       " 'spous',\n",
       " 'yellowston',\n",
       " 'primari',\n",
       " 'refer',\n",
       " 'plan',\n",
       " 'trip',\n",
       " 'addit',\n",
       " 'decis',\n",
       " 'found',\n",
       " 'aspect',\n",
       " 'mount',\n",
       " 'hike',\n",
       " 'forest',\n",
       " 'exhibit',\n",
       " 'visitor',\n",
       " 'grant',\n",
       " 'villag',\n",
       " 'faith',\n",
       " 'inn',\n",
       " 'overlook',\n",
       " 'basin',\n",
       " 'lake',\n",
       " 'hotel',\n",
       " 'inspir',\n",
       " 'high',\n",
       " 'fli',\n",
       " 'jackson',\n",
       " 'fish',\n",
       " 'may',\n",
       " 'additon',\n",
       " 'map',\n",
       " 'sourc',\n",
       " 'area',\n",
       " 'improv',\n",
       " 'condens',\n",
       " 'easi',\n",
       " 'accur',\n",
       " 'afford',\n",
       " 'posit',\n",
       " 'stepper',\n",
       " 'portabl',\n",
       " 'seller',\n",
       " 'respond',\n",
       " 'request',\n",
       " 'help',\n",
       " 'fragil',\n",
       " 'complaint',\n",
       " 'heat',\n",
       " 'caraf',\n",
       " 'hot',\n",
       " 'brew',\n",
       " 'coffe',\n",
       " 'sinc',\n",
       " 'warm',\n",
       " 'plate',\n",
       " 'microwav',\n",
       " 'overflow',\n",
       " 'diet',\n",
       " 'healthi',\n",
       " 'constip',\n",
       " 'dehydr',\n",
       " 'repel',\n",
       " 'breath',\n",
       " 'bandwagon',\n",
       " 'shoe',\n",
       " 'nba',\n",
       " 'live',\n",
       " '2004',\n",
       " 'shot',\n",
       " 'button',\n",
       " 'battl',\n",
       " 'realist',\n",
       " 'game',\n",
       " 'worth',\n",
       " 'penni',\n",
       " 'stop',\n",
       " 'forward',\n",
       " 'novel',\n",
       " 'fine',\n",
       " 'fey',\n",
       " 'tarot',\n",
       " 'local',\n",
       " 'amazoncom',\n",
       " 'conform',\n",
       " 'standard',\n",
       " 'structur',\n",
       " 'deck',\n",
       " 'usabl',\n",
       " 'isnt',\n",
       " 'faeri',\n",
       " 'number',\n",
       " 'suit',\n",
       " 'match',\n",
       " 'normal',\n",
       " 'confus',\n",
       " 'inconsist',\n",
       " 'here',\n",
       " 'south',\n",
       " 'havent',\n",
       " 'chanc',\n",
       " 'size',\n",
       " 'compar',\n",
       " 'two',\n",
       " 'rate',\n",
       " 'girl',\n",
       " 'hors',\n",
       " 'black',\n",
       " 'beauti',\n",
       " 'nation',\n",
       " 'velvet',\n",
       " 'spent',\n",
       " 'saturday',\n",
       " 'receiv',\n",
       " 'jumbo',\n",
       " 'fri',\n",
       " 'cutter',\n",
       " 'hour',\n",
       " 'ago',\n",
       " 'potato',\n",
       " 'broke',\n",
       " 'low',\n",
       " 'qualiti',\n",
       " 'plastic',\n",
       " 'avoid',\n",
       " 'cost',\n",
       " 'televis',\n",
       " 'wave',\n",
       " 'futur',\n",
       " '1930',\n",
       " 'nazi',\n",
       " 'germani',\n",
       " 'resourc',\n",
       " 'leader',\n",
       " 'field',\n",
       " 'program',\n",
       " 'discuss',\n",
       " 'involv',\n",
       " 'due',\n",
       " 'novelti',\n",
       " 'inordin',\n",
       " 'radio',\n",
       " 'medium',\n",
       " 'war',\n",
       " 'justifi',\n",
       " 'vital',\n",
       " 'rare',\n",
       " 'footag',\n",
       " 'recov',\n",
       " 'east',\n",
       " 'archiv',\n",
       " 'interview',\n",
       " 'surviv',\n",
       " 'particip',\n",
       " 'must',\n",
       " 'learn',\n",
       " 'earli',\n",
       " 'undon',\n",
       " 'buddi',\n",
       " 'name',\n",
       " 'jona',\n",
       " 'weezer',\n",
       " 'huge',\n",
       " 'rever',\n",
       " 'away',\n",
       " 'band',\n",
       " 'download',\n",
       " 'burn',\n",
       " 'child',\n",
       " 'least',\n",
       " 'four',\n",
       " 'avid',\n",
       " 'undoubtedli',\n",
       " 'adult',\n",
       " 'writer',\n",
       " 'creat',\n",
       " 'imageri',\n",
       " 'drew',\n",
       " 'truli',\n",
       " 'classic',\n",
       " 'outtak',\n",
       " 'funni',\n",
       " 'coupl',\n",
       " 'con',\n",
       " 'breakout',\n",
       " 'beat',\n",
       " 'ton',\n",
       " '5',\n",
       " 'restart',\n",
       " 'longer',\n",
       " 'ide',\n",
       " 'bounti',\n",
       " 'vulner',\n",
       " 'navig',\n",
       " 'part',\n",
       " 'unlock',\n",
       " 'cheat',\n",
       " 'fun',\n",
       " 'hunter',\n",
       " 'clone',\n",
       " 'star',\n",
       " 'arriv',\n",
       " 'slightli',\n",
       " 'worn',\n",
       " 'true',\n",
       " 'journey',\n",
       " 'accept',\n",
       " 'news',\n",
       " 'pend',\n",
       " 'intrins',\n",
       " 'fate',\n",
       " 'god',\n",
       " 'beyond',\n",
       " 'diseas',\n",
       " 'eventu',\n",
       " 'question',\n",
       " 'anytim',\n",
       " 'machin',\n",
       " 'toaster',\n",
       " 'toast',\n",
       " '10',\n",
       " '4',\n",
       " 'english',\n",
       " 'muffin',\n",
       " 'halv',\n",
       " 'booklet',\n",
       " 'contain',\n",
       " 'convers',\n",
       " 'tabl',\n",
       " 'convect',\n",
       " 'market',\n",
       " 'oster',\n",
       " 'poirot',\n",
       " 'extraordinari',\n",
       " 'man',\n",
       " 'hardli',\n",
       " 'five',\n",
       " 'feet',\n",
       " 'inch',\n",
       " 'digniti',\n",
       " 'head',\n",
       " 'egg',\n",
       " 'perch',\n",
       " 'side',\n",
       " 'stiff',\n",
       " 'militari',\n",
       " 'mysteri',\n",
       " 'celebr',\n",
       " 'belgian',\n",
       " 'detect',\n",
       " 'hercul',\n",
       " 'reason',\n",
       " 'famou',\n",
       " 'member',\n",
       " 'polic',\n",
       " 'forc',\n",
       " 'ww',\n",
       " 'england',\n",
       " 'refuge',\n",
       " 'remain',\n",
       " 'capt',\n",
       " 'hast',\n",
       " 'narrat',\n",
       " 'starsthi',\n",
       " 'kindl',\n",
       " 'version',\n",
       " 'mandolin',\n",
       " 'slicer',\n",
       " 'oper',\n",
       " 'slice',\n",
       " 'tougher',\n",
       " 'veget',\n",
       " 'squash',\n",
       " 'onion',\n",
       " 'slide',\n",
       " 'break',\n",
       " 'half',\n",
       " 'stick',\n",
       " 'knive',\n",
       " 'processor',\n",
       " 'anoth',\n",
       " 'site',\n",
       " 'instal',\n",
       " 'configur',\n",
       " 'beleiv',\n",
       " '45',\n",
       " 'later',\n",
       " 'linksi',\n",
       " '30',\n",
       " 'paramet',\n",
       " 'broadband',\n",
       " 'slow',\n",
       " 'disconnect',\n",
       " 'cabl',\n",
       " 'modem',\n",
       " 'directli',\n",
       " 'fast',\n",
       " 'tech',\n",
       " 'thru',\n",
       " 'messag',\n",
       " 'guess',\n",
       " 'mail',\n",
       " 'box',\n",
       " 'full',\n",
       " 'punish',\n",
       " 'wahlberg',\n",
       " 'homeless',\n",
       " 'seen',\n",
       " 'armi',\n",
       " 'shock',\n",
       " 'dead',\n",
       " 'cri',\n",
       " 'cracker',\n",
       " 'lol',\n",
       " 'pictur',\n",
       " 'costum',\n",
       " 'daughter',\n",
       " 'itchi',\n",
       " 'sharp',\n",
       " 'els',\n",
       " 'joy',\n",
       " ...]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d9e5104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_punct(a):        \n",
    "    return a.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def clean_stop(a):\n",
    "    a_token=a.split()\n",
    "    \n",
    "    str1=\"\"\n",
    "    for tk in a_token:\n",
    "        if tk in stopwords:\n",
    "            continue\n",
    "        else:\n",
    "            str1 += tk+\" \"\n",
    "            \n",
    "    return str1\n",
    "\n",
    "def clean_stem(a):\n",
    "    a_token=a.split()\n",
    "    \n",
    "    str1=\"\"\n",
    "    for tk in a_token:\n",
    "            str1 += porter.stem(tk)+\" \"\n",
    "            \n",
    "    return str1\n",
    "\n",
    "def clean_words(a):\n",
    "    a_token==a.split()\n",
    "    \n",
    "     str1=\"\"\n",
    "    for tk in a_token:\n",
    "        if tk in words\n",
    "            str1 += tk+\" \"\n",
    "            \n",
    "    return str1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38e13135",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "261d4298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(text_list, model, tokenizer):\n",
    "  \"\"\"\n",
    "  To get array with predicted probabilities for 0 - instructions, 1- ingredients classes \n",
    "  for each paragraph in the list of strings\n",
    "  :param text_list: list[str]\n",
    "  :param model: transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForSequenceClassification\n",
    "  :param tokenizer: transformers.models.distilbert.tokenization_distilbert.DistilBertTokenizer\n",
    "  :return res: numpy.ndarray\n",
    "  \"\"\"\n",
    "     \n",
    "  encodings = tokenizer(text_list, max_length=128, truncation=True, padding=True)\n",
    "  dataset = tf.data.Dataset.from_tensor_slices((dict(encodings))) \n",
    "  preds = model.predict(dataset.batch(1)).logits\n",
    "  res = tf.nn.softmax(preds, axis=1).numpy()\n",
    "    \n",
    "  return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "858334c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82a200d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-20 22:39:36.399809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-20 22:39:36.409830: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-20 22:39:36.410182: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-20 22:39:36.411135: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-20 22:39:36.436766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-20 22:39:36.437229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-20 22:39:36.437553: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-20 22:39:36.946913: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-20 22:39:36.947231: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-20 22:39:36.947491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-20 22:39:36.947805: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3368 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "2022-02-20 22:39:37.016377: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_layer_norm', 'vocab_projector', 'vocab_transform', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'dropout_19', 'pre_classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Stiri Pozitive---\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7f996e74dbe0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.Socket(zmq.PUSH) at 0x7f996e74dbe0>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "Stiri_pozitive/34.txt  0.872146725654602  0.12785325944423676\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "Stiri_pozitive/12.txt  0.048085760325193405  0.9519142508506775\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "Stiri_pozitive/11.txt  0.8349164128303528  0.1650836020708084\n",
      "Stiri_pozitive/31.txt  0.6838696599006653  0.31613031029701233\n",
      "Stiri_pozitive/3.txt  0.03524366021156311  0.9647563099861145\n",
      "Stiri_pozitive/17.txt  0.20906874537467957  0.790931224822998\n",
      "Stiri_pozitive/8.txt  0.9908909201622009  0.009109060280025005\n",
      "Stiri_pozitive/10.txt  0.023661816492676735  0.9763382077217102\n",
      "Stiri_pozitive/16.txt  0.2668288052082062  0.7331711649894714\n",
      "Stiri_pozitive/23.txt  0.07948201894760132  0.9205180406570435\n",
      "Stiri_pozitive/33.txt  0.8209945559501648  0.1790054738521576\n",
      "Stiri_pozitive/30.txt  0.9195529222488403  0.08044710010290146\n",
      "Stiri_pozitive/18.txt  0.9582503437995911  0.04174968600273132\n",
      "Stiri_pozitive/7.txt  0.7440017461776733  0.2559982240200043\n",
      "Stiri_pozitive/22.txt  0.059257764369249344  0.9407422542572021\n",
      "Stiri_pozitive/6.txt  0.6887273788452148  0.31127259135246277\n",
      "Stiri_pozitive/29.txt  0.6837140917778015  0.3162858784198761\n",
      "Stiri_pozitive/26.txt  0.007922006770968437  0.9920780062675476\n",
      "Stiri_pozitive/27.txt  0.5081570148468018  0.49184301495552063\n",
      "Stiri_pozitive/28.txt  0.9459571242332458  0.05404283106327057\n",
      "Stiri_pozitive/0.txt  0.9558603167533875  0.04413974657654762\n",
      "Stiri_pozitive/35.txt  0.707682728767395  0.2923172116279602\n",
      "Stiri_pozitive/1.txt  0.9849774837493896  0.015022535808384418\n",
      "Stiri_pozitive/21.txt  0.015144789591431618  0.9848552346229553\n",
      "Stiri_pozitive/9.txt  0.9773828387260437  0.022617213428020477\n",
      "Stiri_pozitive/24.txt  0.9713085293769836  0.028691476210951805\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_445609/3524917329.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparagrafe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparagrafe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_445609/484372620.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(text_list, model, tokenizer)\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mencodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencodings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m   \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1980\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1981\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1982\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1983\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1984\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    955\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2954\u001b[0m       (graph_function,\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2956\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1854\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    500\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     55\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "model.load_weights('./checkpoints2/my_checkpoint')\n",
    "\n",
    "count=0;\n",
    "\n",
    "pozitive_total=0\n",
    "negative_total=0\n",
    "pozitive_corecte=0\n",
    "negative_corecte=0\n",
    "neutre_total=0\n",
    "neutre_corecte=0\n",
    "txtfiles = []\n",
    "for file in glob.glob(\"Stiri_pozitive/*.txt\"):\n",
    "    txtfiles.append(file)\n",
    "    \n",
    "\n",
    "print(\"---Stiri Pozitive---\")    \n",
    "for file in txtfiles:\n",
    "    with open(file, \"r\") as fis:\n",
    "        stire = fis.read()\n",
    "        \n",
    "        paragrafe=str(stire).split('\\n')\n",
    "        while(\"\" in paragrafe) :\n",
    "            paragrafe.remove(\"\")\n",
    "            \n",
    "        paragrafe=list(map(clean_punct, paragrafe))\n",
    "        paragrafe=list(map(clean_stop, paragrafe))\n",
    "        #paragrafe=list(map(clean_stem, paragrafe))       \n",
    "        \n",
    "        results=[]\n",
    "        for i in range(0,len(paragrafe)):\n",
    "            result=predict_proba(paragrafe, model, tokenizer).tolist()[0]   \n",
    "            results.append(result)\n",
    "       \n",
    "        sum_poz=0\n",
    "        sum_neg=0\n",
    "        nr_paragrafe=0\n",
    "        for i in range(0,len(paragrafe)):\n",
    "                nr_paragrafe+=1*len(str(paragrafe[i]).split())\n",
    "                sum_poz+=results[i][0]*len(str(paragrafe[i]).split())\n",
    "                sum_neg+=results[i][1]*len(str(paragrafe[i]).split())\n",
    "\n",
    "        print(str(file)+\"  \"+str(sum_neg/nr_paragrafe)+\"  \"+str(sum_poz/nr_paragrafe))\n",
    "        \n",
    "        pozitive_total+=1\n",
    "        if (sum_poz/nr_paragrafe) > 0.5 :\n",
    "            pozitive_corecte+=1\n",
    "\n",
    "     \n",
    "print(\"Total stiri pozitive : \"+str(pozitive_total))\n",
    "print(\"Corecte stiri pozitive : \"+str(pozitive_corecte)) \n",
    "print(\"Acuratete Pozitive: \"+str(pozitive_corecte/pozitive_total))\n",
    "print(\"\")    \n",
    "\n",
    "txtfiles = []\n",
    "for file in glob.glob(\"Stiri_negative/*.txt\"):\n",
    "    txtfiles.append(file)\n",
    "print(\"---Stiri Negative---\")    \n",
    "for file in txtfiles:\n",
    "    with open(file, \"r\") as fis:\n",
    "        stire = fis.read()\n",
    "        \n",
    "        paragrafe=str(stire).split('\\n')\n",
    "        while(\"\" in paragrafe) :\n",
    "            paragrafe.remove(\"\")\n",
    "            \n",
    "        paragrafe=list(map(clean_punct, paragrafe))\n",
    "        paragrafe=list(map(clean_stop, paragrafe))\n",
    "        #paragrafe=list(map(clean_stem, paragrafe))\n",
    "        \n",
    "        results=[]\n",
    "        for i in range(0,len(paragrafe)):\n",
    "            result=predict_proba(paragrafe, model, tokenizer).tolist()[0]   \n",
    "            results.append(result)\n",
    "    \n",
    "        \n",
    "        sum_poz=0\n",
    "        sum_neg=0\n",
    "        nr_paragrafe=0\n",
    "        for i in range(0,len(paragrafe)):\n",
    "                nr_paragrafe+=1*len(str(paragrafe[i]).split())\n",
    "                sum_poz+=results[i][0]*len(str(paragrafe[i]).split())\n",
    "                sum_neg+=results[i][1]*len(str(paragrafe[i]).split())\n",
    "        \n",
    "        print(str(file)+\"  \"+str(sum_neg/nr_paragrafe)+\"  \"+str(sum_poz/nr_paragrafe))\n",
    "        \n",
    "        negative_total+=1\n",
    "        if sum_neg/nr_paragrafe < 0.5:\n",
    "            negative_corecte+=1\n",
    "    \n",
    "print(\"Total stiri negative : \"+str(negative_total))\n",
    "print(\"Corecte stiri negative : \"+str(negative_corecte)) \n",
    "print(\"Acuratete negative: \"+str(negative_corecte/negative_total))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Total stiri :\" + str(pozitive_total+negative_total+neutre_total))\n",
    "print(\"Total corecte :\" + str(pozitive_corecte+negative_corecte+neutre_corecte))\n",
    "print(\"Acuratete :\" + str((pozitive_corecte+negative_corecte+neutre_corecte)/(pozitive_total+negative_total+neutre_total)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db38bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "model.load_weights('./checkpoints/my_checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e619d829",
   "metadata": {},
   "outputs": [],
   "source": [
    "string1 = [\"this is good\"]\n",
    "predict_proba(string1, model, tokenizer).tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd5198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf. __version__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e8780c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
