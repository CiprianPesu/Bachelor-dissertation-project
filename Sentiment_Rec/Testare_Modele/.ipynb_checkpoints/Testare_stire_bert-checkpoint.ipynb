{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47a6b618",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "import glob\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba45a0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import AutoTokenizer, DistilBertModel\n",
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4aa7c965",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3112d87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter=PorterStemmer()\n",
    "stopwords=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d9e5104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_punct(a):        \n",
    "    return a.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def clean_stop(a):\n",
    "    a_token=a.split()\n",
    "    \n",
    "    str1=\"\"\n",
    "    for tk in a_token:\n",
    "        if tk in stopwords:\n",
    "            continue\n",
    "        else:\n",
    "            str1 += tk+\" \"\n",
    "            \n",
    "    return str1\n",
    "\n",
    "def clean_stem(a):\n",
    "    a_token=a.split()\n",
    "    \n",
    "    str1=\"\"\n",
    "    for tk in a_token:\n",
    "            str1 += porter.stem(tk)+\" \"\n",
    "            \n",
    "    return str1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3876137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def tokenize_function(example):\n",
    "            tok = tokenizer(example, padding=\"max_length\", truncation=True)\n",
    "            return tok['input_ids'], tok['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "11ef2a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.df.iloc[idx]\n",
    "        return {\n",
    "            'input_ids' : torch.tensor(item['input_ids']).to(device),\n",
    "            'attention_mask' : torch.tensor(item['attention_mask']).to(device),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d37fc177",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, out_feat=2):\n",
    "        super().__init__()\n",
    "        self.bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.cls = nn.Linear(768, out_feat)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    " \n",
    "        pooled_output = outputs.last_hidden_state[:,0,:] #[batch_dim, token_dim, ...] [CLS]\n",
    "        logits = self.cls(pooled_output)\n",
    "        return F.softmax(logits, dim=1)\n",
    "\n",
    "    def freeze_until_layer(self, n):\n",
    "      for name, param in self.named_parameters():\n",
    "        if str(n) in name:\n",
    "          break\n",
    "\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    def print_layers(self):\n",
    "      total_nr_w = 0\n",
    "      trainable_nr_w = 0\n",
    "      for name, param in self.named_parameters():\n",
    "        nr_w = np.prod(param.size())\n",
    "        total_nr_w += nr_w\n",
    "        if param.requires_grad:\n",
    "          trainable_nr_w += nr_w\n",
    "        print('{}\\t{}\\t\\t\\t{}'.format(param.requires_grad, nr_w ,name))\n",
    "      print('The network has {} parameters, out of which {} ({:.1f}%) are trainable.'.format(total_nr_w, trainable_nr_w, trainable_nr_w / total_nr_w * 100))\n",
    "\n",
    "model = Classifier(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a1e91d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model):\n",
    "  # Pregatim o modalitate de stocare a datelor pentru evaluare\n",
    "    eval_outputs = []\n",
    "  # Trecem modelul in modul train\n",
    "    model.eval()\n",
    "  ########### Evaluation Loop #############\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, total=len(test_loader)):\n",
    "            outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'])\n",
    "\n",
    "            outputs = outputs.cpu() ## copy-ing the outputs from CUDA to CPU\n",
    "            outputs = np.argmax(outputs, axis=1)\n",
    "\n",
    "            eval_outputs += outputs.tolist()\n",
    "     \n",
    "    cnt_poz=0\n",
    "    cnt_neg=0\n",
    "    for i in eval_outputs:\n",
    "        if i == 1:\n",
    "            cnt_poz++\n",
    "        else:\n",
    "            cnt_neg++\n",
    "    print(eval_outputs)\n",
    "    print(cnt_poz/cnt_neg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "752bc5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=torch.load(\"classfication_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760e156b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Stiri Pozitive---\n",
      "---Stiri Negative---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 9/9 [00:02<00:00,  3.52it/s]\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 13/13 [00:03<00:00,  3.87it/s]\n",
      "  0%|                                                    | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 18/18 [00:05<00:00,  3.41it/s]\n",
      "  0%|                                                    | 0/41 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 41/41 [00:11<00:00,  3.59it/s]\n",
      "  0%|                                                    | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 11/11 [00:03<00:00,  3.44it/s]\n",
      "  0%|                                                    | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 20/20 [00:05<00:00,  3.37it/s]\n",
      "  0%|                                                     | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 9/9 [00:02<00:00,  3.19it/s]\n",
      "  0%|                                                    | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 13/13 [00:03<00:00,  3.36it/s]\n",
      "  0%|                                                    | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 18/18 [00:04<00:00,  3.92it/s]\n",
      "  0%|                                                    | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████████████████████████████████▊      | 12/14 [00:03<00:00,  4.00it/s]"
     ]
    }
   ],
   "source": [
    "count=0;\n",
    "\n",
    "pozitive_total=0\n",
    "negative_total=0\n",
    "pozitive_corecte=0\n",
    "negative_corecte=0\n",
    "neutre_total=0\n",
    "neutre_corecte=0\n",
    "txtfiles = []\n",
    "for file in glob.glob(\"Stiri_pozitive/*.txtt\"):\n",
    "    txtfiles.append(file)\n",
    "    \n",
    "print(\"---Stiri Pozitive---\")    \n",
    "for file in txtfiles:\n",
    "    with open(file, \"r\") as fis:\n",
    "        stire = fis.read()\n",
    "        \n",
    "        paragrafe=str(stire).split('\\n')\n",
    "        while(\"\" in paragrafe) :\n",
    "            paragrafe.remove(\"\")\n",
    "            \n",
    "        paragrafe=map(clean_punct, paragrafe)\n",
    "        paragrafe=map(clean_stop, paragrafe)\n",
    "        #paragrafe=list(map(clean_stem, paragrafe))\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "       \n",
    "        df=pd.DataFrame(paragrafe,columns =['text'])\n",
    "        df['input_ids'], df['attention_mask'] = zip(*df['text'].map(tokenize_function))\n",
    "        \n",
    "        test_set = Dataset(df)\n",
    "        test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=False)\n",
    "        \n",
    "        evaluate(model)\n",
    "        \n",
    "        #sum_poz=0\n",
    "        #sum_neg=0\n",
    "        #for i in range(0,len(paragrafe)):\n",
    "        #    sum_neg+=results[i][0]\n",
    "        #    sum_poz+=results[i][1]\n",
    "        \n",
    "        #print(str(file)+\"  \"+str(sum_neg/len(paragrafe))+\"     \"+str(sum_poz/len(paragrafe)))\n",
    "        \n",
    "        #pozitive_total+=1\n",
    "        #if sum_neg < sum_poz:\n",
    "        #    pozitive_corecte+=1\n",
    "\n",
    "     \n",
    "#print(\"Total stiri pozitive : \"+str(pozitive_total))\n",
    "#print(\"Corecte stiri pozitive : \"+str(pozitive_corecte)) \n",
    "#print(\"Acuratete Pozitive: \"+str(pozitive_corecte/pozitive_total))\n",
    "#print(\"\")    \n",
    "\n",
    "\n",
    "txtfiles = []\n",
    "for file in glob.glob(\"Stiri_negative/*.txt\"):\n",
    "    txtfiles.append(file)\n",
    "print(\"---Stiri Negative---\")    \n",
    "for file in txtfiles:\n",
    "    with open(file, \"r\") as fis:\n",
    "        stire = fis.read()\n",
    "        \n",
    "        paragrafe=str(stire).split('\\n')\n",
    "        while(\"\" in paragrafe) :\n",
    "            paragrafe.remove(\"\")\n",
    "            \n",
    "        paragrafe=list(map(clean_punct, paragrafe))\n",
    "        paragrafe=list(map(clean_stop, paragrafe))\n",
    "        paragrafe=list(map(clean_stem, paragrafe))\n",
    "        \n",
    "        df=pd.DataFrame(paragrafe,columns =['text'])\n",
    "        df['input_ids'], df['attention_mask'] = zip(*df['text'].map(tokenize_function))\n",
    "        \n",
    "        test_set = Dataset(df)\n",
    "        test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=False)\n",
    "        \n",
    "        evaluate(model)\n",
    "        \n",
    "    \n",
    "print(\"Total stiri negative : \"+str(negative_total))\n",
    "print(\"Corecte stiri negative : \"+str(negative_corecte)) \n",
    "print(\"Acuratete negative: \"+str(negative_corecte/negative_total))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Total stiri :\" + str(pozitive_total+negative_total+neutre_total))\n",
    "print(\"Total corecte :\" + str(pozitive_corecte+negative_corecte+neutre_corecte))\n",
    "print(\"Acuratete :\" + str((pozitive_corecte+negative_corecte+neutre_corecte)/(pozitive_total+negative_total+neutre_total)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4d8131fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cnnafrican elephants drastically dwindled targ...</td>\n",
       "      <td>[101, 13229, 10354, 14735, 2078, 16825, 21040,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>researchers used genetic testing ivory shipmen...</td>\n",
       "      <td>[101, 6950, 2109, 7403, 5604, 11554, 24636, 82...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>team tested 4000 elephant tusks 49 different s...</td>\n",
       "      <td>[101, 2136, 7718, 20143, 10777, 10722, 6711, 2...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>study establishes familial connections elephan...</td>\n",
       "      <td>[101, 2817, 21009, 6904, 4328, 6632, 2140, 726...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>kind dna detective work expose tactics employe...</td>\n",
       "      <td>[101, 2785, 6064, 6317, 2147, 14451, 9887, 484...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>methods showing us handful networks behind maj...</td>\n",
       "      <td>[101, 4725, 4760, 2149, 9210, 6125, 2369, 3484...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>linking elephant family members</td>\n",
       "      <td>[101, 11383, 10777, 2155, 2372, 102, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>drawing connections separate seizures ivory ma...</td>\n",
       "      <td>[101, 5059, 7264, 3584, 25750, 11554, 2081, 88...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>study builds previous work published wasser co...</td>\n",
       "      <td>[101, 2817, 16473, 3025, 2147, 2405, 2001, 804...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>new research broadened dna analysis find eleph...</td>\n",
       "      <td>[101, 2047, 2470, 5041, 6675, 6064, 4106, 2424...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>three networks established 2018 study involved...</td>\n",
       "      <td>[101, 2093, 6125, 2511, 2760, 2817, 2920, 2116...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>youre trying match one tusk pair low chance ma...</td>\n",
       "      <td>[101, 2115, 2063, 2667, 2674, 2028, 10722, 671...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>special agent john brown study coauthor crimin...</td>\n",
       "      <td>[101, 2569, 4005, 2198, 2829, 2817, 28155, 143...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>targeted populations</td>\n",
       "      <td>[101, 9416, 7080, 102, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>tusks came forest elephants savannah elephants...</td>\n",
       "      <td>[101, 10722, 6711, 2015, 2234, 3224, 16825, 10...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>savannah elephants roam across grassy plains b...</td>\n",
       "      <td>[101, 10891, 16825, 25728, 2408, 22221, 8575, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>tusks shipped ports different countries fact s...</td>\n",
       "      <td>[101, 10722, 6711, 2015, 12057, 8831, 2367, 30...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>elephant populations females tend remain famil...</td>\n",
       "      <td>[101, 10777, 7080, 3801, 7166, 3961, 2155, 217...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>identifying close relatives indicates poachers...</td>\n",
       "      <td>[101, 12151, 2485, 9064, 7127, 13433, 15395, 2...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>criminal strategy makes much harder authoritie...</td>\n",
       "      <td>[101, 4735, 5656, 3084, 2172, 6211, 4614, 2650...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>holding traffickers accountable</td>\n",
       "      <td>[101, 3173, 4026, 11451, 26771, 102, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>small group smuggling networks likely ones res...</td>\n",
       "      <td>[101, 2235, 2177, 19535, 6125, 3497, 3924, 362...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>lot movement make sale ivory illegal many coun...</td>\n",
       "      <td>[101, 2843, 2929, 2191, 5096, 11554, 6206, 211...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>17year span study also showed networks shifted...</td>\n",
       "      <td>[101, 2459, 29100, 8487, 2817, 2036, 3662, 612...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>criminals often linked one ivory shipment seiz...</td>\n",
       "      <td>[101, 12290, 2411, 5799, 2028, 11554, 22613, 1...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>extinction species ecological collapse wildlif...</td>\n",
       "      <td>[101, 14446, 2427, 12231, 7859, 6870, 11626, 2...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "0   cnnafrican elephants drastically dwindled targ...   \n",
       "1   researchers used genetic testing ivory shipmen...   \n",
       "2   team tested 4000 elephant tusks 49 different s...   \n",
       "3   study establishes familial connections elephan...   \n",
       "4   kind dna detective work expose tactics employe...   \n",
       "5   methods showing us handful networks behind maj...   \n",
       "6                    linking elephant family members    \n",
       "7   drawing connections separate seizures ivory ma...   \n",
       "8   study builds previous work published wasser co...   \n",
       "9   new research broadened dna analysis find eleph...   \n",
       "10  three networks established 2018 study involved...   \n",
       "11  youre trying match one tusk pair low chance ma...   \n",
       "12  special agent john brown study coauthor crimin...   \n",
       "13                              targeted populations    \n",
       "14  tusks came forest elephants savannah elephants...   \n",
       "15  savannah elephants roam across grassy plains b...   \n",
       "16  tusks shipped ports different countries fact s...   \n",
       "17  elephant populations females tend remain famil...   \n",
       "18  identifying close relatives indicates poachers...   \n",
       "19  criminal strategy makes much harder authoritie...   \n",
       "20                   holding traffickers accountable    \n",
       "21  small group smuggling networks likely ones res...   \n",
       "22  lot movement make sale ivory illegal many coun...   \n",
       "23  17year span study also showed networks shifted...   \n",
       "24  criminals often linked one ivory shipment seiz...   \n",
       "25  extinction species ecological collapse wildlif...   \n",
       "\n",
       "                                            input_ids  \\\n",
       "0   [101, 13229, 10354, 14735, 2078, 16825, 21040,...   \n",
       "1   [101, 6950, 2109, 7403, 5604, 11554, 24636, 82...   \n",
       "2   [101, 2136, 7718, 20143, 10777, 10722, 6711, 2...   \n",
       "3   [101, 2817, 21009, 6904, 4328, 6632, 2140, 726...   \n",
       "4   [101, 2785, 6064, 6317, 2147, 14451, 9887, 484...   \n",
       "5   [101, 4725, 4760, 2149, 9210, 6125, 2369, 3484...   \n",
       "6   [101, 11383, 10777, 2155, 2372, 102, 0, 0, 0, ...   \n",
       "7   [101, 5059, 7264, 3584, 25750, 11554, 2081, 88...   \n",
       "8   [101, 2817, 16473, 3025, 2147, 2405, 2001, 804...   \n",
       "9   [101, 2047, 2470, 5041, 6675, 6064, 4106, 2424...   \n",
       "10  [101, 2093, 6125, 2511, 2760, 2817, 2920, 2116...   \n",
       "11  [101, 2115, 2063, 2667, 2674, 2028, 10722, 671...   \n",
       "12  [101, 2569, 4005, 2198, 2829, 2817, 28155, 143...   \n",
       "13  [101, 9416, 7080, 102, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "14  [101, 10722, 6711, 2015, 2234, 3224, 16825, 10...   \n",
       "15  [101, 10891, 16825, 25728, 2408, 22221, 8575, ...   \n",
       "16  [101, 10722, 6711, 2015, 12057, 8831, 2367, 30...   \n",
       "17  [101, 10777, 7080, 3801, 7166, 3961, 2155, 217...   \n",
       "18  [101, 12151, 2485, 9064, 7127, 13433, 15395, 2...   \n",
       "19  [101, 4735, 5656, 3084, 2172, 6211, 4614, 2650...   \n",
       "20  [101, 3173, 4026, 11451, 26771, 102, 0, 0, 0, ...   \n",
       "21  [101, 2235, 2177, 19535, 6125, 3497, 3924, 362...   \n",
       "22  [101, 2843, 2929, 2191, 5096, 11554, 6206, 211...   \n",
       "23  [101, 2459, 29100, 8487, 2817, 2036, 3662, 612...   \n",
       "24  [101, 12290, 2411, 5799, 2028, 11554, 22613, 1...   \n",
       "25  [101, 14446, 2427, 12231, 7859, 6870, 11626, 2...   \n",
       "\n",
       "                                       attention_mask  \n",
       "0   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "5   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "6   [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "7   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "8   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "9   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "10  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "11  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "12  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "13  [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "14  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "15  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "16  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "17  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "18  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "19  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "20  [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "21  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "22  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "23  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "24  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "25  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47494e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
