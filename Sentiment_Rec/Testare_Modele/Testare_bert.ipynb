{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47a6b618",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "import glob\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.stem import PorterStemmer\n",
    "from bs4 import BeautifulSoup\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, DistilBertModel\n",
    "from transformers import AdamW\n",
    "import transformers\n",
    "from transformers import DistilBertTokenizer\n",
    "from transformers import TFDistilBertForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3112d87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "porter=PorterStemmer()\n",
    "stopwords=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y']\n",
    "import pickle\n",
    "with open('tokenizer-200.pickle', 'rb') as handle:\n",
    "    words = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "529e44eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "words_list=json.loads(words.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3031ba41",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=json.loads(words_list[\"config\"][\"word_counts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e22b5999",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=list(y.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "091fcdbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apron',\n",
       " 'show',\n",
       " 'plu',\n",
       " 'fade',\n",
       " 'quickli',\n",
       " 'expos',\n",
       " 'big',\n",
       " 'deal',\n",
       " 'within',\n",
       " 'week',\n",
       " 'use',\n",
       " 'around',\n",
       " 'fabric',\n",
       " 'stitch',\n",
       " 'attach',\n",
       " 'one',\n",
       " 'pocket',\n",
       " 'toward',\n",
       " 'center',\n",
       " 'start',\n",
       " 'rip',\n",
       " 'not',\n",
       " 'stay',\n",
       " 'tear',\n",
       " 'loos',\n",
       " 'appear',\n",
       " 'snag',\n",
       " 'someth',\n",
       " 'move',\n",
       " 'shop',\n",
       " 'expect',\n",
       " 'otherwis',\n",
       " 'ok',\n",
       " 'price',\n",
       " 'no',\n",
       " 'dock',\n",
       " 'web',\n",
       " 'cam',\n",
       " 'featur',\n",
       " 'ever',\n",
       " 'buy',\n",
       " 'thisth',\n",
       " 'zoom',\n",
       " 'stuck',\n",
       " 'never',\n",
       " 'set',\n",
       " 'foot',\n",
       " 'danc',\n",
       " 'studio',\n",
       " 'might',\n",
       " 'enjoy',\n",
       " 'video',\n",
       " 'anyon',\n",
       " '2',\n",
       " 'lesson',\n",
       " 'would',\n",
       " 'feel',\n",
       " 'wast',\n",
       " 'money',\n",
       " 'among',\n",
       " 'us',\n",
       " 'hasnt',\n",
       " 'notic',\n",
       " 'centuri',\n",
       " 'human',\n",
       " 'seem',\n",
       " 'becom',\n",
       " 'insul',\n",
       " 'selfinvolv',\n",
       " 'less',\n",
       " 'mind',\n",
       " 'other',\n",
       " 'hope',\n",
       " 'lynn',\n",
       " 'truss',\n",
       " 'offer',\n",
       " 'humor',\n",
       " 'observ',\n",
       " 'subject',\n",
       " 'along',\n",
       " 'tongueincheek',\n",
       " 'way',\n",
       " 'possibl',\n",
       " 'bit',\n",
       " 'insight',\n",
       " 'thought',\n",
       " 'thing',\n",
       " 'lot',\n",
       " 'whine',\n",
       " 'person',\n",
       " 'petti',\n",
       " 'annoy',\n",
       " 'credit',\n",
       " 'write',\n",
       " 'style',\n",
       " 'extrem',\n",
       " 'witti',\n",
       " 'find',\n",
       " 'laugh',\n",
       " 'loud',\n",
       " 'read',\n",
       " 'passag',\n",
       " 'overal',\n",
       " 'felt',\n",
       " 'like',\n",
       " 'listen',\n",
       " 'gripe',\n",
       " 'friend',\n",
       " 'afternoon',\n",
       " 'hand',\n",
       " 'bring',\n",
       " 'forefront',\n",
       " 'imperson',\n",
       " 'weve',\n",
       " 'there',\n",
       " 'much',\n",
       " 'point',\n",
       " 'rais',\n",
       " 'awar',\n",
       " 'alreadi',\n",
       " 'look',\n",
       " 'substanc',\n",
       " 'ms',\n",
       " 'told',\n",
       " 'talk',\n",
       " 'saw',\n",
       " 'come',\n",
       " 'compat',\n",
       " 'basic',\n",
       " 'command',\n",
       " 'control',\n",
       " 'tv',\n",
       " 'volum',\n",
       " 'channel',\n",
       " 'chang',\n",
       " 'muchit',\n",
       " 'cheap',\n",
       " 'good',\n",
       " 'product',\n",
       " 'issu',\n",
       " 'indic',\n",
       " 'remot',\n",
       " 'mode',\n",
       " 'record',\n",
       " 'press',\n",
       " 'arrow',\n",
       " 'doesnt',\n",
       " 'light',\n",
       " 'know',\n",
       " 'absolut',\n",
       " 'rivet',\n",
       " 'mani',\n",
       " 'fascin',\n",
       " 'tale',\n",
       " 'young',\n",
       " 'american',\n",
       " 'endur',\n",
       " 'time',\n",
       " 'great',\n",
       " 'struggl',\n",
       " 'reread',\n",
       " 'outstand',\n",
       " 'work',\n",
       " 'lost',\n",
       " 'mr',\n",
       " 'uri',\n",
       " 'compliment',\n",
       " 'excel',\n",
       " 'gift',\n",
       " 'reader',\n",
       " 'bought',\n",
       " 'son',\n",
       " '4th',\n",
       " 'golf',\n",
       " 'go',\n",
       " 'first',\n",
       " 'real',\n",
       " 'club',\n",
       " 'item',\n",
       " 'put',\n",
       " 'togeth',\n",
       " 'batteri',\n",
       " 'fulli',\n",
       " 'charg',\n",
       " 'excit',\n",
       " 'hop',\n",
       " 'right',\n",
       " 'went',\n",
       " 'nowher',\n",
       " 'pice',\n",
       " 'junk',\n",
       " 'weigh',\n",
       " 'level',\n",
       " 'lawn',\n",
       " 'grass',\n",
       " 'cut',\n",
       " 'previou',\n",
       " 'day',\n",
       " '1',\n",
       " 'back',\n",
       " 'wheel',\n",
       " 'pull',\n",
       " 'simpli',\n",
       " 'power',\n",
       " 'perhap',\n",
       " 'better',\n",
       " 'driveway',\n",
       " 'alot',\n",
       " 'ride',\n",
       " 'deer',\n",
       " 'tractor',\n",
       " 'he',\n",
       " 'year',\n",
       " 'run',\n",
       " 'recommend',\n",
       " 'unless',\n",
       " 'strictli',\n",
       " 'cours',\n",
       " 'made',\n",
       " 'concret',\n",
       " 'though',\n",
       " 'cd',\n",
       " 'pretens',\n",
       " 'sonata',\n",
       " 'anywher',\n",
       " 'near',\n",
       " 'everi',\n",
       " 'song',\n",
       " 'sound',\n",
       " 'pretti',\n",
       " 'titl',\n",
       " 'track',\n",
       " 'usual',\n",
       " 'choru',\n",
       " 'sung',\n",
       " 'album',\n",
       " 'repetit',\n",
       " 'almost',\n",
       " 'varieti',\n",
       " 'differ',\n",
       " 'last',\n",
       " 'instrument',\n",
       " 'dont',\n",
       " 'realli',\n",
       " 'hardcor',\n",
       " 'metal',\n",
       " 'fan',\n",
       " 'happi',\n",
       " 'list',\n",
       " 'book',\n",
       " 'order',\n",
       " 'condit',\n",
       " 'email',\n",
       " 'bob',\n",
       " 'got',\n",
       " 'follow',\n",
       " 'could',\n",
       " 'understood',\n",
       " 'provid',\n",
       " 'proper',\n",
       " 'custom',\n",
       " 'servic',\n",
       " 'origin',\n",
       " 'convinc',\n",
       " 'consid',\n",
       " 'stand',\n",
       " 'behind',\n",
       " 'didnt',\n",
       " 'death',\n",
       " '20',\n",
       " 'cool',\n",
       " 'think',\n",
       " 'breed',\n",
       " 'best',\n",
       " 'introduc',\n",
       " 'amaz',\n",
       " 'take',\n",
       " 'florida',\n",
       " 'master',\n",
       " 'naturalist',\n",
       " 'class',\n",
       " 'extens',\n",
       " 'univers',\n",
       " 'want',\n",
       " 'understand',\n",
       " 'complex',\n",
       " 'divis',\n",
       " 'explan',\n",
       " 'well',\n",
       " 'written',\n",
       " 'approach',\n",
       " 'studi',\n",
       " 'begin',\n",
       " 'wasnt',\n",
       " 'worst',\n",
       " 'movi',\n",
       " 'seeni',\n",
       " 'lame',\n",
       " 'action',\n",
       " 'itin',\n",
       " 'opinion',\n",
       " 'also',\n",
       " 'miscast',\n",
       " 'chri',\n",
       " 'klein',\n",
       " 'herei',\n",
       " 'ace',\n",
       " 'charact',\n",
       " 'belong',\n",
       " 'michael',\n",
       " 'clark',\n",
       " 'duncan',\n",
       " 'play',\n",
       " 'role',\n",
       " 'alway',\n",
       " 'doubt',\n",
       " 'talent',\n",
       " 'wonder',\n",
       " 'bodi',\n",
       " 'didth',\n",
       " 'decent',\n",
       " 'moon',\n",
       " 'entir',\n",
       " 'legend',\n",
       " '175',\n",
       " 'instruct',\n",
       " 'pump',\n",
       " 'cleans',\n",
       " 'mitchel',\n",
       " 'keep',\n",
       " 'wen',\n",
       " 'algebra',\n",
       " 'fundament',\n",
       " 'far',\n",
       " 'cover',\n",
       " 'exampl',\n",
       " 'problem',\n",
       " 'pay',\n",
       " 'brand',\n",
       " 'new',\n",
       " 'shape',\n",
       " 'descript',\n",
       " 'care',\n",
       " 'make',\n",
       " 'sure',\n",
       " 'get',\n",
       " 'badli',\n",
       " 'view',\n",
       " 'life',\n",
       " 'theori',\n",
       " 'percept',\n",
       " 'quit',\n",
       " 'explain',\n",
       " 'meif',\n",
       " 'your',\n",
       " 'open',\n",
       " 'everyth',\n",
       " 'wrong',\n",
       " 'film',\n",
       " 'believ',\n",
       " 'see',\n",
       " 'wont',\n",
       " 'idea',\n",
       " 'present',\n",
       " 'belief',\n",
       " 'mix',\n",
       " 'grand',\n",
       " 'absurd',\n",
       " 'hold',\n",
       " 'merit',\n",
       " 'advantag',\n",
       " 'say',\n",
       " 'quantum',\n",
       " 'physic',\n",
       " 'that',\n",
       " 'bs',\n",
       " 'secondli',\n",
       " 'ive',\n",
       " 'knowledg',\n",
       " 'base',\n",
       " 'im',\n",
       " 'lyle',\n",
       " 'behavior',\n",
       " 'turn',\n",
       " 'storylin',\n",
       " 'howev',\n",
       " 'pregnant',\n",
       " 'father',\n",
       " '15',\n",
       " 'atroci',\n",
       " 'dread',\n",
       " 'circumst',\n",
       " 'maggi',\n",
       " 'protect',\n",
       " 'final',\n",
       " 'reveal',\n",
       " 'truth',\n",
       " 'endi',\n",
       " 'romanc',\n",
       " 'emot',\n",
       " 'connect',\n",
       " 'tragic',\n",
       " 'episod',\n",
       " 'world',\n",
       " 'histori',\n",
       " 'difficult',\n",
       " 'watch',\n",
       " 'grandfath',\n",
       " 'fell',\n",
       " 'german',\n",
       " 'special',\n",
       " 'interest',\n",
       " 'even',\n",
       " 'moor',\n",
       " 'perfect',\n",
       " 'palin',\n",
       " 'scene',\n",
       " 'campaign',\n",
       " 'happen',\n",
       " 'desir',\n",
       " 'win',\n",
       " 'place',\n",
       " 'countri',\n",
       " 'christma',\n",
       " 'sister',\n",
       " 'comput',\n",
       " 'yeah',\n",
       " 'socal',\n",
       " 'hip',\n",
       " 'peopl',\n",
       " 'squeaki',\n",
       " 'clean',\n",
       " 'imagin',\n",
       " 'mean',\n",
       " 'amazon',\n",
       " 'review',\n",
       " 'miss',\n",
       " 'joke',\n",
       " 'materi',\n",
       " 'chose',\n",
       " 'richard',\n",
       " 'chees',\n",
       " 'choos',\n",
       " 'whew',\n",
       " 'late',\n",
       " 'daddi',\n",
       " 'beg',\n",
       " 'ozzi',\n",
       " 'call',\n",
       " 'next',\n",
       " 'door',\n",
       " 'neighbor',\n",
       " 'presenc',\n",
       " 'osbourn',\n",
       " 'famili',\n",
       " 'kind',\n",
       " 'stuff',\n",
       " 'tonight',\n",
       " 'lennon',\n",
       " 'sometim',\n",
       " 'artist',\n",
       " 'need',\n",
       " 'selfright',\n",
       " 'bubbl',\n",
       " 'pop',\n",
       " 'ask',\n",
       " '2007',\n",
       " 'bad',\n",
       " 'pain',\n",
       " 'tri',\n",
       " 'finish',\n",
       " 'author',\n",
       " 'heard',\n",
       " 'needless',\n",
       " 'probabl',\n",
       " 'favorit',\n",
       " 'disappoint',\n",
       " 'tire',\n",
       " 'kingdom',\n",
       " '04',\n",
       " '07',\n",
       " 'plop',\n",
       " '85',\n",
       " 'month',\n",
       " '3',\n",
       " 'replac',\n",
       " '12',\n",
       " 'retail',\n",
       " 'carri',\n",
       " 'buyer',\n",
       " 'bewar',\n",
       " 'stori',\n",
       " 'suspens',\n",
       " 'done',\n",
       " 'without',\n",
       " 'swear',\n",
       " 'scari',\n",
       " 'intens',\n",
       " 'arent',\n",
       " 'kristen',\n",
       " 'sever',\n",
       " 'blue',\n",
       " 'tooth',\n",
       " 'ear',\n",
       " 'piec',\n",
       " 'luck',\n",
       " 'whatsoev',\n",
       " 'total',\n",
       " 'unreli',\n",
       " 'aw',\n",
       " 'everyon',\n",
       " 'water',\n",
       " 'decid',\n",
       " 'impress',\n",
       " 'actual',\n",
       " 'hear',\n",
       " 'clear',\n",
       " 'certainli',\n",
       " 'abl',\n",
       " 'drive',\n",
       " 'handsfre',\n",
       " 'rememb',\n",
       " 'technolog',\n",
       " 'still',\n",
       " 'leav',\n",
       " 'quirk',\n",
       " 'dual',\n",
       " 'monitor',\n",
       " 'tablet',\n",
       " 'geniu',\n",
       " 'technic',\n",
       " 'support',\n",
       " 'said',\n",
       " 'sorri',\n",
       " 'none',\n",
       " 'licens',\n",
       " 'yuck',\n",
       " 'invest',\n",
       " '24',\n",
       " 'graphic',\n",
       " 'useless',\n",
       " 'solut',\n",
       " 'pleas',\n",
       " 'post',\n",
       " 'comment',\n",
       " 'thank',\n",
       " 'farberwar',\n",
       " 'pressur',\n",
       " 'cooker',\n",
       " 'hassl',\n",
       " 'hard',\n",
       " 'tell',\n",
       " 'couldnt',\n",
       " 'exactli',\n",
       " 'long',\n",
       " 'cook',\n",
       " 'food',\n",
       " 'minut',\n",
       " 'end',\n",
       " 'mush',\n",
       " 'took',\n",
       " 'forev',\n",
       " 'pot',\n",
       " 'ran',\n",
       " 'cold',\n",
       " 'wouldnt',\n",
       " 'wait',\n",
       " 'oven',\n",
       " 'instead',\n",
       " 'faster',\n",
       " 'presto',\n",
       " 'love',\n",
       " 'steamer',\n",
       " 'basket',\n",
       " 'regular',\n",
       " 'backpack',\n",
       " 'canon',\n",
       " 'mk',\n",
       " 'ii',\n",
       " 'len',\n",
       " '100',\n",
       " 'macro',\n",
       " 'flash',\n",
       " 'equip',\n",
       " 'incident',\n",
       " 'cf',\n",
       " 'card',\n",
       " 'extra',\n",
       " 'cloth',\n",
       " 'cleaner',\n",
       " 'etc',\n",
       " 'risk',\n",
       " 'scratch',\n",
       " 'bump',\n",
       " 'broken',\n",
       " 'mingl',\n",
       " 'camera',\n",
       " 'tote',\n",
       " 'lens',\n",
       " 'apart',\n",
       " 'plenti',\n",
       " 'storag',\n",
       " 'space',\n",
       " 'comfort',\n",
       " 'pack',\n",
       " 'nice',\n",
       " 'especi',\n",
       " 'front',\n",
       " 'option',\n",
       " 'strap',\n",
       " 'clive',\n",
       " 'cussler',\n",
       " 'render',\n",
       " 'date',\n",
       " 'includ',\n",
       " 'tomb',\n",
       " 'advertis',\n",
       " 'cant',\n",
       " 'blame',\n",
       " 'stupid',\n",
       " 'print',\n",
       " 'came',\n",
       " 'littl',\n",
       " 'guy',\n",
       " 'favor',\n",
       " 'public',\n",
       " 'lead',\n",
       " 'old',\n",
       " 'prim',\n",
       " 'rose',\n",
       " 'path',\n",
       " 'gee',\n",
       " 'aint',\n",
       " 'direct',\n",
       " 'pour',\n",
       " 'bottom',\n",
       " 'manual',\n",
       " 'defect',\n",
       " 'return',\n",
       " 'merced',\n",
       " 'lackey',\n",
       " 'she',\n",
       " 'seri',\n",
       " 'continu',\n",
       " 'third',\n",
       " 'creativ',\n",
       " 'fire',\n",
       " 'conflict',\n",
       " 'develop',\n",
       " 'danger',\n",
       " 'choic',\n",
       " 'free',\n",
       " 'bard',\n",
       " 'welcom',\n",
       " 'magic',\n",
       " 'earlier',\n",
       " 'adventur',\n",
       " 'caveat',\n",
       " 'honest',\n",
       " 'youll',\n",
       " 'top',\n",
       " 'eat',\n",
       " 'maintain',\n",
       " 'health',\n",
       " 'detail',\n",
       " 'inform',\n",
       " 'specif',\n",
       " 'categori',\n",
       " 'text',\n",
       " 'pepper',\n",
       " 'tip',\n",
       " 'suggest',\n",
       " 'amount',\n",
       " 'consumpt',\n",
       " 'potenti',\n",
       " 'highli',\n",
       " 'someon',\n",
       " 'refin',\n",
       " 'menu',\n",
       " 'increas',\n",
       " 'spous',\n",
       " 'yellowston',\n",
       " 'primari',\n",
       " 'refer',\n",
       " 'plan',\n",
       " 'trip',\n",
       " 'addit',\n",
       " 'decis',\n",
       " 'found',\n",
       " 'aspect',\n",
       " 'mount',\n",
       " 'hike',\n",
       " 'forest',\n",
       " 'exhibit',\n",
       " 'visitor',\n",
       " 'grant',\n",
       " 'villag',\n",
       " 'faith',\n",
       " 'inn',\n",
       " 'overlook',\n",
       " 'basin',\n",
       " 'lake',\n",
       " 'hotel',\n",
       " 'inspir',\n",
       " 'high',\n",
       " 'fli',\n",
       " 'jackson',\n",
       " 'fish',\n",
       " 'may',\n",
       " 'additon',\n",
       " 'map',\n",
       " 'sourc',\n",
       " 'area',\n",
       " 'improv',\n",
       " 'condens',\n",
       " 'easi',\n",
       " 'accur',\n",
       " 'afford',\n",
       " 'posit',\n",
       " 'stepper',\n",
       " 'portabl',\n",
       " 'seller',\n",
       " 'respond',\n",
       " 'request',\n",
       " 'help',\n",
       " 'fragil',\n",
       " 'complaint',\n",
       " 'heat',\n",
       " 'caraf',\n",
       " 'hot',\n",
       " 'brew',\n",
       " 'coffe',\n",
       " 'sinc',\n",
       " 'warm',\n",
       " 'plate',\n",
       " 'microwav',\n",
       " 'overflow',\n",
       " 'diet',\n",
       " 'healthi',\n",
       " 'constip',\n",
       " 'dehydr',\n",
       " 'repel',\n",
       " 'breath',\n",
       " 'bandwagon',\n",
       " 'shoe',\n",
       " 'nba',\n",
       " 'live',\n",
       " '2004',\n",
       " 'shot',\n",
       " 'button',\n",
       " 'battl',\n",
       " 'realist',\n",
       " 'game',\n",
       " 'worth',\n",
       " 'penni',\n",
       " 'stop',\n",
       " 'forward',\n",
       " 'novel',\n",
       " 'fine',\n",
       " 'fey',\n",
       " 'tarot',\n",
       " 'local',\n",
       " 'amazoncom',\n",
       " 'conform',\n",
       " 'standard',\n",
       " 'structur',\n",
       " 'deck',\n",
       " 'usabl',\n",
       " 'isnt',\n",
       " 'faeri',\n",
       " 'number',\n",
       " 'suit',\n",
       " 'match',\n",
       " 'normal',\n",
       " 'confus',\n",
       " 'inconsist',\n",
       " 'here',\n",
       " 'south',\n",
       " 'havent',\n",
       " 'chanc',\n",
       " 'size',\n",
       " 'compar',\n",
       " 'two',\n",
       " 'rate',\n",
       " 'girl',\n",
       " 'hors',\n",
       " 'black',\n",
       " 'beauti',\n",
       " 'nation',\n",
       " 'velvet',\n",
       " 'spent',\n",
       " 'saturday',\n",
       " 'receiv',\n",
       " 'jumbo',\n",
       " 'fri',\n",
       " 'cutter',\n",
       " 'hour',\n",
       " 'ago',\n",
       " 'potato',\n",
       " 'broke',\n",
       " 'low',\n",
       " 'qualiti',\n",
       " 'plastic',\n",
       " 'avoid',\n",
       " 'cost',\n",
       " 'televis',\n",
       " 'wave',\n",
       " 'futur',\n",
       " '1930',\n",
       " 'nazi',\n",
       " 'germani',\n",
       " 'resourc',\n",
       " 'leader',\n",
       " 'field',\n",
       " 'program',\n",
       " 'discuss',\n",
       " 'involv',\n",
       " 'due',\n",
       " 'novelti',\n",
       " 'inordin',\n",
       " 'radio',\n",
       " 'medium',\n",
       " 'war',\n",
       " 'justifi',\n",
       " 'vital',\n",
       " 'rare',\n",
       " 'footag',\n",
       " 'recov',\n",
       " 'east',\n",
       " 'archiv',\n",
       " 'interview',\n",
       " 'surviv',\n",
       " 'particip',\n",
       " 'must',\n",
       " 'learn',\n",
       " 'earli',\n",
       " 'undon',\n",
       " 'buddi',\n",
       " 'name',\n",
       " 'jona',\n",
       " 'weezer',\n",
       " 'huge',\n",
       " 'rever',\n",
       " 'away',\n",
       " 'band',\n",
       " 'download',\n",
       " 'burn',\n",
       " 'child',\n",
       " 'least',\n",
       " 'four',\n",
       " 'avid',\n",
       " 'undoubtedli',\n",
       " 'adult',\n",
       " 'writer',\n",
       " 'creat',\n",
       " 'imageri',\n",
       " 'drew',\n",
       " 'truli',\n",
       " 'classic',\n",
       " 'outtak',\n",
       " 'funni',\n",
       " 'coupl',\n",
       " 'con',\n",
       " 'breakout',\n",
       " 'beat',\n",
       " 'ton',\n",
       " '5',\n",
       " 'restart',\n",
       " 'longer',\n",
       " 'ide',\n",
       " 'bounti',\n",
       " 'vulner',\n",
       " 'navig',\n",
       " 'part',\n",
       " 'unlock',\n",
       " 'cheat',\n",
       " 'fun',\n",
       " 'hunter',\n",
       " 'clone',\n",
       " 'star',\n",
       " 'arriv',\n",
       " 'slightli',\n",
       " 'worn',\n",
       " 'true',\n",
       " 'journey',\n",
       " 'accept',\n",
       " 'news',\n",
       " 'pend',\n",
       " 'intrins',\n",
       " 'fate',\n",
       " 'god',\n",
       " 'beyond',\n",
       " 'diseas',\n",
       " 'eventu',\n",
       " 'question',\n",
       " 'anytim',\n",
       " 'machin',\n",
       " 'toaster',\n",
       " 'toast',\n",
       " '10',\n",
       " '4',\n",
       " 'english',\n",
       " 'muffin',\n",
       " 'halv',\n",
       " 'booklet',\n",
       " 'contain',\n",
       " 'convers',\n",
       " 'tabl',\n",
       " 'convect',\n",
       " 'market',\n",
       " 'oster',\n",
       " 'poirot',\n",
       " 'extraordinari',\n",
       " 'man',\n",
       " 'hardli',\n",
       " 'five',\n",
       " 'feet',\n",
       " 'inch',\n",
       " 'digniti',\n",
       " 'head',\n",
       " 'egg',\n",
       " 'perch',\n",
       " 'side',\n",
       " 'stiff',\n",
       " 'militari',\n",
       " 'mysteri',\n",
       " 'celebr',\n",
       " 'belgian',\n",
       " 'detect',\n",
       " 'hercul',\n",
       " 'reason',\n",
       " 'famou',\n",
       " 'member',\n",
       " 'polic',\n",
       " 'forc',\n",
       " 'ww',\n",
       " 'england',\n",
       " 'refuge',\n",
       " 'remain',\n",
       " 'capt',\n",
       " 'hast',\n",
       " 'narrat',\n",
       " 'starsthi',\n",
       " 'kindl',\n",
       " 'version',\n",
       " 'mandolin',\n",
       " 'slicer',\n",
       " 'oper',\n",
       " 'slice',\n",
       " 'tougher',\n",
       " 'veget',\n",
       " 'squash',\n",
       " 'onion',\n",
       " 'slide',\n",
       " 'break',\n",
       " 'half',\n",
       " 'stick',\n",
       " 'knive',\n",
       " 'processor',\n",
       " 'anoth',\n",
       " 'site',\n",
       " 'instal',\n",
       " 'configur',\n",
       " 'beleiv',\n",
       " '45',\n",
       " 'later',\n",
       " 'linksi',\n",
       " '30',\n",
       " 'paramet',\n",
       " 'broadband',\n",
       " 'slow',\n",
       " 'disconnect',\n",
       " 'cabl',\n",
       " 'modem',\n",
       " 'directli',\n",
       " 'fast',\n",
       " 'tech',\n",
       " 'thru',\n",
       " 'messag',\n",
       " 'guess',\n",
       " 'mail',\n",
       " 'box',\n",
       " 'full',\n",
       " 'punish',\n",
       " 'wahlberg',\n",
       " 'homeless',\n",
       " 'seen',\n",
       " 'armi',\n",
       " 'shock',\n",
       " 'dead',\n",
       " 'cri',\n",
       " 'cracker',\n",
       " 'lol',\n",
       " 'pictur',\n",
       " 'costum',\n",
       " 'daughter',\n",
       " 'itchi',\n",
       " 'sharp',\n",
       " 'els',\n",
       " 'joy',\n",
       " ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d9e5104",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_punct(a):        \n",
    "    return a.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def clean_stop(a):\n",
    "    a_token=a.split()\n",
    "    \n",
    "    str1=\"\"\n",
    "    for tk in a_token:\n",
    "        if tk in stopwords:\n",
    "            continue\n",
    "        else:\n",
    "            str1 += tk+\" \"\n",
    "            \n",
    "    return str1\n",
    "\n",
    "def clean_stem(a):\n",
    "    a_token=a.split()\n",
    "    \n",
    "    str1=\"\"\n",
    "    for tk in a_token:\n",
    "            str1 += porter.stem(tk)+\" \"\n",
    "            \n",
    "    return str1\n",
    "\n",
    "def clean_words(a):\n",
    "    a_token=a.split()\n",
    "    str1=\"\"\n",
    "    for tk in a_token:\n",
    "        if tk in words:\n",
    "            str1 += tk+\" \"\n",
    "            \n",
    "    return str1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38e13135",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "261d4298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(text_list, model, tokenizer):\n",
    "  \"\"\"\n",
    "  To get array with predicted probabilities for 0 - instructions, 1- ingredients classes \n",
    "  for each paragraph in the list of strings\n",
    "  :param text_list: list[str]\n",
    "  :param model: transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForSequenceClassification\n",
    "  :param tokenizer: transformers.models.distilbert.tokenization_distilbert.DistilBertTokenizer\n",
    "  :return res: numpy.ndarray\n",
    "  \"\"\"\n",
    "     \n",
    "  encodings = tokenizer(text_list, max_length=128, truncation=True, padding=True)\n",
    "  dataset = tf.data.Dataset.from_tensor_slices((dict(encodings))) \n",
    "  preds = model.predict(dataset.batch(1)).logits\n",
    "  res = tf.nn.softmax(preds, axis=1).numpy()\n",
    "    \n",
    "  return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761b75f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "model.load_weights('./Modele_Bert/checkpoints2/my_checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82a200d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Stiri Pozitive---\n",
      "Stiri_pozitive/34.txt  0.2721865024759213  0.7278135000138323\n",
      "Stiri_pozitive/42.txt  0.543434668317576  0.4565653214693538\n",
      "Stiri_pozitive/12.txt  0.6765518393066218  0.3234481553204686\n",
      "Stiri_pozitive/41.txt  0.4231666070507705  0.576833390582118\n",
      "Stiri_pozitive/11.txt  0.45329244358441495  0.5467075354902994\n",
      "Stiri_pozitive/31.txt  0.393771314953962  0.6062286874986753\n",
      "Stiri_pozitive/3.txt  0.29830853608027236  0.7016914689709405\n",
      "Stiri_pozitive/39.txt  0.37389176963357096  0.626108223574673\n",
      "Stiri_pozitive/17.txt  0.3613206084778843  0.6386793953776804\n",
      "Stiri_pozitive/8.txt  0.2584538269120188  0.7415461794246271\n",
      "Stiri_pozitive/10.txt  0.3751088792254694  0.6248911082395027\n",
      "Stiri_pozitive/16.txt  0.5381797383613709  0.4618202504474547\n",
      "Stiri_pozitive/23.txt  0.2574418468191134  0.7425581676873151\n",
      "Stiri_pozitive/33.txt  0.12578100582415408  0.8742189865747486\n",
      "Stiri_pozitive/30.txt  0.36618891695632627  0.63381107142484\n",
      "Stiri_pozitive/18.txt  0.21299650609422072  0.7870034945938903\n",
      "Stiri_pozitive/7.txt  0.4023737642931629  0.597626227751396\n",
      "Stiri_pozitive/22.txt  0.443577881951526  0.556422102611552\n",
      "Stiri_pozitive/6.txt  0.554755649496227  0.44524435279253\n",
      "Stiri_pozitive/38.txt  0.282167507124585  0.7178324955529675\n",
      "Stiri_pozitive/29.txt  0.18520480552988666  0.8147951725990542\n",
      "Stiri_pozitive/26.txt  0.43133625379642065  0.5686637602746487\n",
      "Stiri_pozitive/27.txt  0.23000608182379179  0.7699939172448856\n",
      "Stiri_pozitive/28.txt  0.2659862432642034  0.7340137531289785\n",
      "Stiri_pozitive/0.txt  0.3228321617427491  0.6771678619086743\n",
      "Stiri_pozitive/35.txt  0.06758331050098475  0.9324167044035027\n",
      "Stiri_pozitive/1.txt  0.33016624031653113  0.6698337568772129\n",
      "Stiri_pozitive/36.txt  0.3482348734619025  0.6517651321162089\n",
      "Stiri_pozitive/37.txt  0.38065287702268474  0.6193471214940979\n",
      "Stiri_pozitive/21.txt  0.6944819709485244  0.30551801824461927\n",
      "Stiri_pozitive/9.txt  0.3043035856827542  0.6956964122344177\n",
      "Stiri_pozitive/24.txt  0.4216563976212773  0.5783436145156969\n",
      "Stiri_pozitive/13.txt  0.1860949623468885  0.8139050357381675\n",
      "Stiri_pozitive/14.txt  0.5700221803727562  0.4299778083912875\n",
      "Stiri_pozitive/40.txt  0.3510766806893728  0.6489233217388392\n",
      "Stiri_pozitive/2.txt  0.25817180599034245  0.7418282163225942\n",
      "Stiri_pozitive/25.txt  0.42660062061920945  0.5733993796205029\n",
      "Stiri_pozitive/43.txt  0.5706227203391576  0.4293772842442052\n",
      "Stiri_pozitive/5.txt  0.3941652724774045  0.6058347203629371\n",
      "Stiri_pozitive/32.txt  0.4712415577916606  0.5287584503538693\n",
      "Stiri_pozitive/4.txt  0.1738780776701438  0.8261219137598976\n",
      "Stiri_pozitive/20.txt  0.45809451521005634  0.5419054731872166\n",
      "Stiri_pozitive/19.txt  0.10418835383281795  0.8958116564023144\n",
      "Stiri_pozitive/15.txt  0.19137665878574783  0.808623339571437\n",
      "Total stiri pozitive : 44\n",
      "Corecte stiri pozitive : 37\n",
      "Acuratete Pozitive: 0.8409090909090909\n",
      "\n",
      "---Stiri Negative---\n",
      "Stiri_negative/34.txt  0.6059322730258658  0.3940677143909313\n",
      "Stiri_negative/42.txt  0.5915162401443178  0.4084837677084248\n",
      "Stiri_negative/12.txt  0.5607684930851846  0.43923150490511925\n",
      "Stiri_negative/41.txt  0.6267379638067511  0.3732620432756321\n",
      "Stiri_negative/45.txt  0.6038045090957437  0.39619548907730634\n",
      "Stiri_negative/11.txt  0.46560853031346866  0.534391463971366\n",
      "Stiri_negative/31.txt  0.6958368063446576  0.30416319936324476\n",
      "Stiri_negative/3.txt  0.5383977540981267  0.46160223681388585\n",
      "Stiri_negative/39.txt  0.6965692105487097  0.3034307891577212\n",
      "Stiri_negative/17.txt  0.735811877265468  0.2641881194074473\n",
      "Stiri_negative/8.txt  0.7142337499924938  0.2857662478604411\n",
      "Stiri_negative/10.txt  0.8893181896635464  0.11068181973837671\n",
      "Stiri_negative/16.txt  0.6365144844721012  0.3634855084506331\n",
      "Stiri_negative/23.txt  0.6680214911973702  0.3319785144778767\n",
      "Stiri_negative/33.txt  0.9365043135850053  0.06349564124584983\n",
      "Stiri_negative/30.txt  0.7718560516660989  0.228143956962713\n",
      "Stiri_negative/18.txt  0.6280832710210233  0.37191672489734184\n",
      "Stiri_negative/44.txt  0.6110661170384037  0.3889338831735139\n",
      "Stiri_negative/7.txt  0.7500950731218178  0.24990492123674524\n",
      "Stiri_negative/22.txt  0.5562549673393369  0.4437450394460133\n",
      "Stiri_negative/6.txt  0.7622061612377377  0.23779384935239578\n",
      "Stiri_negative/38.txt  0.7564682047703478  0.24353180039342087\n",
      "Stiri_negative/29.txt  0.5827574683472196  0.4172425342347235\n",
      "Stiri_negative/26.txt  0.5066182951706478  0.4933817033357215\n",
      "Stiri_negative/27.txt  0.7321548674071644  0.26784514022393174\n",
      "Stiri_negative/28.txt  0.567461399759488  0.43253857497869924\n",
      "Stiri_negative/0.txt  0.6438775678744186  0.35612244650318814\n",
      "Stiri_negative/46.txt  0.8027755813099521  0.1972244172221675\n",
      "Stiri_negative/35.txt  0.7418235255822223  0.25817647534495175\n",
      "Stiri_negative/1.txt  0.6961725983704472  0.3038274116683671\n",
      "Stiri_negative/36.txt  0.6549192060381174  0.3450807798281312\n",
      "Stiri_negative/37.txt  0.6760221192204217  0.32397787702093656\n",
      "Stiri_negative/21.txt  0.7229538022461584  0.27704619456393026\n",
      "Stiri_negative/9.txt  0.5251593370977719  0.47484065854035956\n",
      "Stiri_negative/24.txt  0.37527582065470644  0.6247241781298387\n",
      "Stiri_negative/13.txt  0.617907032812665  0.3820929735551128\n",
      "Stiri_negative/14.txt  0.8390422803865969  0.1609577204785962\n",
      "Stiri_negative/40.txt  0.6315996998594638  0.36840030666414614\n",
      "Stiri_negative/2.txt  0.5986973278019577  0.4013027033023536\n",
      "Stiri_negative/25.txt  0.5404375247078613  0.4595624899943766\n",
      "Stiri_negative/43.txt  0.6139217736728086  0.3860782293933192\n",
      "Stiri_negative/5.txt  0.5428917703646663  0.45710823177485854\n",
      "Stiri_negative/32.txt  0.6944812439728615  0.30551875628704245\n",
      "Stiri_negative/4.txt  0.47416509024798875  0.5258349120679001\n",
      "Stiri_negative/20.txt  0.8127558145408997  0.18724419502620349\n",
      "Stiri_negative/19.txt  0.587020387223076  0.4129796155202339\n",
      "Stiri_negative/15.txt  0.43070212073418773  0.569297896468259\n",
      "Total stiri negative : 47\n",
      "Corecte stiri negative : 43\n",
      "Acuratete negative: 0.9148936170212766\n",
      "\n",
      "Total stiri :91\n",
      "Total corecte :80\n",
      "Acuratete :0.8791208791208791\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "\n",
    "count=0;\n",
    "\n",
    "pozitive_total=0\n",
    "negative_total=0\n",
    "pozitive_corecte=0\n",
    "negative_corecte=0\n",
    "neutre_total=0\n",
    "neutre_corecte=0\n",
    "txtfiles = []\n",
    "for file in glob.glob(\"Stiri_pozitive/*.txt\"):\n",
    "    txtfiles.append(file)\n",
    "    \n",
    "\n",
    "print(\"---Stiri Pozitive---\")    \n",
    "for file in txtfiles:\n",
    "    with open(file, \"r\") as fis:\n",
    "        stire = fis.read()\n",
    "        \n",
    "        paragrafe=str(stire).split('\\n')\n",
    "        while(\"\" in paragrafe) :\n",
    "            paragrafe.remove(\"\")\n",
    "            \n",
    "        paragrafe=list(map(clean_punct, paragrafe))\n",
    "        paragrafe=list(map(clean_stop, paragrafe))\n",
    "        paragrafe=list(map(clean_stem, paragrafe))       \n",
    "        paragrafe=list(map(clean_words, paragrafe))  \n",
    "        \n",
    "        results=predict_proba(paragrafe, model, tokenizer).tolist() \n",
    "       \n",
    "        sum_poz=0\n",
    "        sum_neg=0\n",
    "        nr_paragrafe=0\n",
    "        for i in range(0,len(paragrafe)):\n",
    "                nr_paragrafe+=1*len(str(paragrafe[i]).split())\n",
    "                sum_poz+=results[i][1]*len(str(paragrafe[i]).split())\n",
    "                sum_neg+=results[i][0]*len(str(paragrafe[i]).split())\n",
    "\n",
    "        procent_neg=sum_neg/nr_paragrafe-0.05\n",
    "        procent_poz=sum_poz/nr_paragrafe+0.05\n",
    "\n",
    "        print(str(file)+\"  \"+str(procent_neg)+\"  \"+str(procent_poz))\n",
    "        \n",
    "        pozitive_total+=1\n",
    "        if procent_poz > 0.5 :\n",
    "            pozitive_corecte+=1\n",
    "\n",
    "     \n",
    "print(\"Total stiri pozitive : \"+str(pozitive_total))\n",
    "print(\"Corecte stiri pozitive : \"+str(pozitive_corecte)) \n",
    "print(\"Acuratete Pozitive: \"+str(pozitive_corecte/pozitive_total))\n",
    "print(\"\")    \n",
    "\n",
    "txtfiles = []\n",
    "for file in glob.glob(\"Stiri_negative/*.txt\"):\n",
    "    txtfiles.append(file)\n",
    "print(\"---Stiri Negative---\")    \n",
    "for file in txtfiles:\n",
    "    with open(file, \"r\") as fis:\n",
    "        stire = fis.read()\n",
    "        \n",
    "        paragrafe=str(stire).split('\\n')\n",
    "        while(\"\" in paragrafe) :\n",
    "            paragrafe.remove(\"\")\n",
    "            \n",
    "        paragrafe=list(map(clean_punct, paragrafe))\n",
    "        paragrafe=list(map(clean_stop, paragrafe))\n",
    "        paragrafe=list(map(clean_stem, paragrafe))       \n",
    "        paragrafe=list(map(clean_words, paragrafe)) \n",
    "        \n",
    "        results=predict_proba(paragrafe, model, tokenizer).tolist()  \n",
    "\n",
    "    \n",
    "        sum_poz=0\n",
    "        sum_neg=0\n",
    "        nr_paragrafe=0\n",
    "        for i in range(0,len(paragrafe)):\n",
    "                nr_paragrafe+=1*len(str(paragrafe[i]).split())\n",
    "                sum_poz+=results[i][1]*len(str(paragrafe[i]).split())\n",
    "                sum_neg+=results[i][0]*len(str(paragrafe[i]).split())\n",
    "\n",
    "        \n",
    "        procent_neg=sum_neg/nr_paragrafe-0.05\n",
    "        procent_poz=sum_poz/nr_paragrafe+0.05\n",
    "\n",
    "        print(str(file)+\"  \"+str(procent_neg)+\"  \"+str(procent_poz))\n",
    "        \n",
    "        negative_total+=1\n",
    "        if procent_poz < 0.5:\n",
    "            negative_corecte+=1\n",
    "    \n",
    "print(\"Total stiri negative : \"+str(negative_total))\n",
    "print(\"Corecte stiri negative : \"+str(negative_corecte)) \n",
    "print(\"Acuratete negative: \"+str(negative_corecte/negative_total))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Total stiri :\" + str(pozitive_total+negative_total+neutre_total))\n",
    "print(\"Total corecte :\" + str(pozitive_corecte+negative_corecte+neutre_corecte))\n",
    "print(\"Acuratete :\" + str((pozitive_corecte+negative_corecte+neutre_corecte)/(pozitive_total+negative_total+neutre_total)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db38bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "model.load_weights('./checkpoints/my_checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e619d829",
   "metadata": {},
   "outputs": [],
   "source": [
    "string1 = [\"this is not good\",\n",
    "          \"This is good\",\n",
    "          \"This is shit\",\n",
    "          \"This is the best decision of my life\"]\n",
    "paragrafe=list(map(clean_punct, string1))\n",
    "paragrafe=list(map(clean_stop, string1))\n",
    "paragrafe=list(map(clean_stem, string1))       \n",
    "paragrafe=list(map(clean_words, string1)) \n",
    "\n",
    "predict_proba(string1, model, tokenizer).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd5198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf. __version__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e8780c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
