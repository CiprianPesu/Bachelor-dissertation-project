{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "222e4d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow_hub as hub\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import tensorflow_text\n",
    "import spacy\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef0d75c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ciprian/.local/lib/python3.9/site-packages/numpy/lib/arraysetops.py:580: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../Datasets/train-200.csv\",index_col=[0])\n",
    "#df=df.sample(n=10000)\n",
    "#df=df.drop([\"Unnamed: 0\"],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7acf679f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>this sound track was it paints the in your min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>reading a lot of reviews saying that this is t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>this soundtrack is my favorite music of all ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>i truly like this soundtrack and i enjoy video...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>if played the you know how divine the music ev...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          2  this sound track was it paints the in your min...\n",
       "1          2  reading a lot of reviews saying that this is t...\n",
       "2          2  this soundtrack is my favorite music of all ha...\n",
       "3          2  i truly like this soundtrack and i enjoy video...\n",
       "4          2  if played the you know how divine the music ev..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38d11fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"../Datasets/train-200.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b87b37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=df[\"text\"]\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc88aafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y']\n",
      "['no', 'nor', 'not', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", 'don', \"that'll\"]\n"
     ]
    }
   ],
   "source": [
    "stopwords=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y']\n",
    "stopwords_neg=['no', 'nor', 'not', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\" , \"don\", \"that'll\"]\n",
    "print(stopwords)\n",
    "print(stopwords_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb5e72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts=[]\n",
    "for text in texts:\n",
    "    doc=nlp(text)\n",
    "    filtered_tokens = [token for token in doc if not stopwords]\n",
    "    tokenized_texts.append(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092ce367",
   "metadata": {},
   "outputs": [],
   "source": [
    "max=0\n",
    "for text in tokenized_texts:\n",
    "    if len(text)>max:\n",
    "        max=len(text)\n",
    "print(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181fcf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max=256\n",
    "expanded_tokenized_texts=[]\n",
    "for text in tokenized_texts:\n",
    "    text=text[:max] + [nlp(\"null\")]*(max - len(text))\n",
    "    expanded_tokenized_texts.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d61f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_texts=[]\n",
    "for text in  expanded_tokenized_texts:\n",
    "    vectors=[]\n",
    "    for word in text:\n",
    "        vectors.append((numpy.array(word.vector)))\n",
    "    normalized_texts.append(numpy.array(vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9287857b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(numpy.array(normalized_texts).shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb429136",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "type_one_hot = OneHotEncoder(sparse=False).fit_transform(\n",
    "  df.sentiment.to_numpy().reshape(-1, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825c7f25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0920cb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "train_news, test_news, y_train, y_test =\\\n",
    "  train_test_split(\n",
    "    normalized_texts, \n",
    "    type_one_hot, \n",
    "    test_size=.1, \n",
    "    random_state=RANDOM_SEED\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e7bb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_news=np.array(train_news)\n",
    "test_news=np.array(test_news)\n",
    "y_train=np.array(y_train)\n",
    "y_test=np.array(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fc2b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_news[1].shape)\n",
    "print(test_news.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40710f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_news.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa912ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "model.add(keras.layers.cell(units, return_sequences=False))\n",
    "\n",
    "model.add(\n",
    "  keras.layers.Dense(\n",
    "    units=128,\n",
    "    input_shape=(train_news[1].shape[0],train_news[1].shape[1]),\n",
    "    activation='relu'\n",
    "  )\n",
    ")\n",
    "model.add(\n",
    "  keras.layers.Dropout(rate=0.5)\n",
    ")\n",
    "\n",
    "model.add(keras.layers.GlobalMaxPool1D())\n",
    "\n",
    "model.add(\n",
    "  keras.layers.Dense(\n",
    "    units=32,\n",
    "    activation='relu'\n",
    "  )\n",
    ")\n",
    "model.add(\n",
    "  keras.layers.Dropout(rate=0.5)\n",
    ")\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(2, activation='softmax'))\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer=keras.optimizers.Adam(0.001),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b0b28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bfe533",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_news, y_train, \n",
    "    epochs=30, \n",
    "    batch_size=32, \n",
    "    validation_split=0.1,\n",
    "    verbose=1, \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e25a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='train loss'),\n",
    "plt.plot(history.history['val_loss'], label='val loss'),\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Cross-entropy loss\"),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60379d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    plt.plot(history.history['accuracy'], label='train accuracy'),\n",
    "    plt.plot(history.history['val_accuracy'], label='val accuracy'),\n",
    "    plt.xlabel(\"epoch\"),\n",
    "    plt.ylabel(\"accuracy\"),\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee27c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_news, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9ca0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./Model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b25133d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e47f9d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
