{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "222e4d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow_hub as hub\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import tensorflow_text\n",
    "import spacy\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef0d75c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "header_list=[\"sentiment\",\"text\"]\n",
    "df = pd.read_csv(\"train_fix.csv\",names=header_list)\n",
    "df=df.sample(n=10000)\n",
    "#df=df.drop([\"news_title\",\"url\",\"reddit_title\"],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7acf679f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         sentiment                                               text\n",
      "777110           0  This gate is not functional with baseboards th...\n",
      "27862            1  This flag was given to me as a gift from my wi...\n",
      "1042995          1  This came in a brand new box perfect for my si...\n",
      "906474           1  I purchased this crock pot as a Christmas gift...\n",
      "983466           1  This one is probably easier for some to digest...\n",
      "...            ...                                                ...\n",
      "908995           0  I've been satisfied with the earbuds that came...\n",
      "319486           1  I don't know what album this tracklist is from...\n",
      "1026365          0  This is by far and away the worst DVD I have e...\n",
      "1033105          0  I got this book for Christmas last year and I ...\n",
      "391973           1  This was one of my favorite movies of the 90s....\n",
      "\n",
      "[10000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d11fee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b87b37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=df[\"text\"]\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fb5e72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts=[]\n",
    "for text in texts:\n",
    "    doc=nlp(text)\n",
    "    filtered_tokens = [token for token in doc if not token.is_stop if not token.is_punct]\n",
    "    tokenized_texts.append(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d29679",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "092ce367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119\n"
     ]
    }
   ],
   "source": [
    "max=0\n",
    "for text in tokenized_texts:\n",
    "    if len(text)>max:\n",
    "        max=len(text)\n",
    "print(max)\n",
    "\n",
    "max=256\n",
    "expanded_tokenized_texts=[]\n",
    "for text in tokenized_texts:\n",
    "    text=text[:max] + [nlp(\"null\")]*(max - len(text))\n",
    "    expanded_tokenized_texts.append(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72d61f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_texts=[]\n",
    "for text in  expanded_tokenized_texts:\n",
    "    vectors=[]\n",
    "    for word in text:\n",
    "        vectors.append((numpy.array(word.vector)))\n",
    "    normalized_texts.append(numpy.array(vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9287857b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 256, 96)\n"
     ]
    }
   ],
   "source": [
    "print(numpy.array(normalized_texts).shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb429136",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "type_one_hot = OneHotEncoder(sparse=False).fit_transform(\n",
    "  df.sentiment.to_numpy().reshape(-1, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825c7f25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0920cb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "train_news, test_news, y_train, y_test =\\\n",
    "  train_test_split(\n",
    "    normalized_texts, \n",
    "    type_one_hot, \n",
    "    test_size=.1, \n",
    "    random_state=RANDOM_SEED\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27e7bb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_news=np.array(train_news)\n",
    "test_news=np.array(test_news)\n",
    "y_train=np.array(y_train)\n",
    "y_test=np.array(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10fc2b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 96)\n",
      "(1000, 256, 96)\n",
      "(9000, 2)\n",
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train_news[1].shape)\n",
    "print(test_news.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f40710f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 256, 96)\n"
     ]
    }
   ],
   "source": [
    "print(train_news.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfa912ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-04 21:55:31.916604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-04 21:55:36.757501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-04 21:55:36.759062: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-04 21:55:36.809450: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-11-04 21:55:36.811481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-04 21:55:36.812934: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-04 21:55:36.814156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-04 21:56:20.187532: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-04 21:56:20.242821: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-04 21:56:20.243957: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-11-04 21:56:20.244850: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3372 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1050 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.api._v2.keras.layers' has no attribute 'cell'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_497217/2441436159.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m model.add(\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'keras.api._v2.keras.layers' has no attribute 'cell'"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "model.add(keras.layers.cell(units, return_sequences=False))\n",
    "\n",
    "model.add(\n",
    "  keras.layers.Dense(\n",
    "    units=96,\n",
    "    input_shape=(train_news[1].shape[0],train_news[1].shape[1]),\n",
    "    activation='relu'\n",
    "  )\n",
    ")\n",
    "model.add(\n",
    "  keras.layers.Dropout(rate=0.5)\n",
    ")\n",
    "\n",
    "model.add(keras.layers.GlobalMaxPool1D())\n",
    "\n",
    "model.add(\n",
    "  keras.layers.Dense(\n",
    "    units=32,\n",
    "    activation='relu'\n",
    "  )\n",
    ")\n",
    "model.add(\n",
    "  keras.layers.Dropout(rate=0.5)\n",
    ")\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(2, activation='softmax'))\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer=keras.optimizers.Adam(0.001),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b0b28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bfe533",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_news, y_train, \n",
    "    epochs=30, \n",
    "    batch_size=32, \n",
    "    validation_split=0.1,\n",
    "    verbose=1, \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e25a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='train loss'),\n",
    "plt.plot(history.history['val_loss'], label='val loss'),\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"Cross-entropy loss\"),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60379d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "    plt.plot(history.history['accuracy'], label='train accuracy'),\n",
    "    plt.plot(history.history['val_accuracy'], label='val accuracy'),\n",
    "    plt.xlabel(\"epoch\"),\n",
    "    plt.ylabel(\"accuracy\"),\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee27c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_news, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9ca0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./Model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b25133d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e47f9d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
