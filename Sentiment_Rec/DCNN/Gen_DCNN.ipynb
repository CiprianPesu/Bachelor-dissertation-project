{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be9be947",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from keras.models import model_from_yaml\n",
    "from keras.models import model_from_json\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f923feda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee1bf0b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Datasets/Stem-Cuvinte-Eliminate/train-punct-stop-stem-200.csv\")\n",
    "df=df.dropna()\n",
    "df=df.sample(3000000)\n",
    "#df=df.drop([\"news_title\",\"url\",\"reddit_title\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63179b14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2633442</th>\n",
       "      <td>0</td>\n",
       "      <td>second review content somewher present horribl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887608</th>\n",
       "      <td>2</td>\n",
       "      <td>easi guy everyon hate pari obvious go look fau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1273233</th>\n",
       "      <td>2</td>\n",
       "      <td>josh groban fan not enjoy cd romant song beaut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1126739</th>\n",
       "      <td>2</td>\n",
       "      <td>without doubt best album ever compar gilmour r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2107033</th>\n",
       "      <td>0</td>\n",
       "      <td>got vacuum januari 2008 chose vacuum great rev...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment                                               text\n",
       "2633442          0  second review content somewher present horribl...\n",
       "887608           2  easi guy everyon hate pari obvious go look fau...\n",
       "1273233          2  josh groban fan not enjoy cd romant song beaut...\n",
       "1126739          2  without doubt best album ever compar gilmour r...\n",
       "2107033          0  got vacuum januari 2008 chose vacuum great rev..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a7e515d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment    3000000\n",
       "text         3000000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae56d43d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_clean = df.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbafbe51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_labels = df.sentiment.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "079e68f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2633442    second review content somewher present horribl...\n",
       "887608     easi guy everyon hate pari obvious go look fau...\n",
       "1273233    josh groban fan not enjoy cd romant song beaut...\n",
       "1126739    without doubt best album ever compar gilmour r...\n",
       "2107033    got vacuum januari 2008 chose vacuum great rev...\n",
       "                                 ...                        \n",
       "2153686    happen sound qualiti horribl lyric could pen a...\n",
       "3107329    subject excel feel realism good mani point see...\n",
       "1550473    ive use book sinc bought instal window xp ive ...\n",
       "2407267    limp bizkit worst rock rap group ever use term...\n",
       "731857     big fan talk head david byrn everyth els heard...\n",
       "Name: text, Length: 3000000, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90cdd408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 2, ..., 2, 0, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d0afa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels_good=[]\n",
    "for i in data_labels:\n",
    "    if i==2:\n",
    "        data_labels_good.append(1)\n",
    "    else:\n",
    "        data_labels_good.append(0)\n",
    "\n",
    "data_labels_good=np.array(data_labels_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e6c50b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 1, 0, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_labels_good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c5cdaab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(data_labels_good)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "025f1b08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('tokenizer-200.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66dd7fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_phrase_len=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd2e30cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(data_clean)\n",
    "X_train = pad_sequences(X_train, maxlen = max_phrase_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea1502b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb62c6f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class DCNN(tf.keras.Model):\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 emb_dim=128,\n",
    "                 nb_filters=50,\n",
    "                 FFN_units=512,\n",
    "                 nb_classes=2,\n",
    "                 dropout_rate=0.1,\n",
    "                 training=False,\n",
    "                 name=\"dcnn\"):\n",
    "        super(DCNN, self).__init__(name=name)\n",
    "        self.embedding = layers.Embedding(vocab_size,\n",
    "                                          emb_dim)\n",
    "        self.bigram = layers.Conv1D(filters=nb_filters,\n",
    "                                    kernel_size=2,\n",
    "                                    padding=\"valid\",\n",
    "                                    activation=\"relu\")\n",
    "        self.pool_1 = layers.GlobalMaxPool1D()\n",
    "        self.trigram = layers.Conv1D(filters=nb_filters,\n",
    "                                     kernel_size=3,\n",
    "                                     padding=\"valid\",\n",
    "                                     activation=\"relu\")\n",
    "        self.pool_2 = layers.GlobalMaxPool1D()\n",
    "        self.fourgram = layers.Conv1D(filters=nb_filters,\n",
    "                                      kernel_size=4,\n",
    "                                      padding=\"valid\",\n",
    "                                      activation=\"relu\")\n",
    "        \n",
    "        self.pool_3 = layers.GlobalMaxPool1D()\n",
    "        self.dense_1 = layers.Dense(units=FFN_units, activation=\"relu\")\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        if nb_classes == 2:\n",
    "            self.last_dense = layers.Dense(units=1,\n",
    "                                           activation=\"sigmoid\")\n",
    "        else:\n",
    "            self.last_dense = layers.Dense(units=nb_classes,\n",
    "                                           activation=\"softmax\")\n",
    "    \n",
    "    def call(self, inputs, training):\n",
    "        x = self.embedding(inputs)\n",
    "        x_1 = self.bigram(x)\n",
    "        x_1 = self.pool_1(x_1)\n",
    "        x_2 = self.trigram(x)\n",
    "        x_2 = self.pool_2(x_2)\n",
    "        x_3 = self.fourgram(x)\n",
    "        x_3 = self.pool_3(x_3)\n",
    "\n",
    "        merged = tf.concat([x_1, x_2, x_3], axis=-1) # (batch_size, 3 * nb_filters)\n",
    "        merged = self.dense_1(merged)\n",
    "        merged = self.dropout(merged, training)\n",
    "        output = self.last_dense(merged)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b2d0faa9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#VOCAB_SIZE = 16384\n",
    "VOCAB_SIZE = 8192\n",
    "EMB_DIM = 200\n",
    "NB_FILTERS = 100\n",
    "FFN_UNITS = 256\n",
    "NB_CLASSES = len(set(data_labels_good))\n",
    "\n",
    "DROPOUT_RATE = 0.5\n",
    "\n",
    "BATCH_SIZE = 265\n",
    "NB_EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7dbb21eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Dcnn = DCNN(vocab_size=VOCAB_SIZE,\n",
    "            emb_dim=EMB_DIM,\n",
    "            nb_filters=NB_FILTERS,\n",
    "            FFN_units=FFN_UNITS,\n",
    "            nb_classes=NB_CLASSES,\n",
    "            dropout_rate=DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "362ae115",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if NB_CLASSES == 2:\n",
    "    Dcnn.compile(loss=\"binary_crossentropy\",\n",
    "                 optimizer=\"adam\",\n",
    "                 metrics=[\"accuracy\"])\n",
    "else:\n",
    "    Dcnn.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                 optimizer=\"adam\",\n",
    "                 metrics=[\"sparse_categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "276dd631",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-18 18:04:28.224932: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 1228800000 exceeds 10% of free system memory.\n",
      "2022-02-18 18:04:28.935271: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-18 18:04:30.509288: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9057/9057 [==============================] - 545s 60ms/step - loss: 0.2498 - accuracy: 0.8979 - val_loss: 0.2253 - val_accuracy: 0.9086\n",
      "Epoch 2/3\n",
      "   3/9057 [..............................] - ETA: 8:20 - loss: 0.2409 - accuracy: 0.9044"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-18 18:13:33.557290: W tensorflow/core/common_runtime/bfc_allocator.cc:272] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.49GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2022-02-18 18:13:33.569845: W tensorflow/core/common_runtime/bfc_allocator.cc:272] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.49GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n",
      "2022-02-18 18:13:33.581925: W tensorflow/core/common_runtime/bfc_allocator.cc:272] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.49GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9057/9057 [==============================] - 565s 62ms/step - loss: 0.2078 - accuracy: 0.9181 - val_loss: 0.2207 - val_accuracy: 0.9111\n",
      "Epoch 3/3\n",
      "9057/9057 [==============================] - 572s 63ms/step - loss: 0.1817 - accuracy: 0.9298 - val_loss: 0.2259 - val_accuracy: 0.9112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f62e2e7bb80>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dcnn.fit(X_train,\n",
    "         data_labels_good,\n",
    "         batch_size=BATCH_SIZE,\n",
    "         validation_split=0.2,\n",
    "         shuffle=True,\n",
    "         verbose=1,\n",
    "         epochs=NB_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97ed3761",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"dcnn\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  1638400   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              multiple                  40100     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            multiple                  60100     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob multiple                  0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            multiple                  80100     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  77056     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  257       \n",
      "=================================================================\n",
      "Total params: 1,896,013\n",
      "Trainable params: 1,896,013\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Dcnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21cfae7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-18 18:45:43.520023: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./Model-200-stem/assets\n"
     ]
    }
   ],
   "source": [
    "Dcnn.save(\"./Model-200-stem/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d9b65bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_68661/3547954151.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_yaml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model.yaml\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0myaml_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0myaml_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_yaml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mto_json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   2416\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mJSON\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2417\u001b[0m     \"\"\"\n\u001b[0;32m-> 2418\u001b[0;31m     \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2419\u001b[0m     return json.dumps(\n\u001b[1;32m   2420\u001b[0m         model_config, default=json_utils.get_json_type, **kwargs)\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_updated_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2374\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mkeras_version\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2376\u001b[0;31m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2377\u001b[0m     model_config = {\n\u001b[1;32m   2378\u001b[0m         \u001b[0;34m'class_name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mget_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2385\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2386\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2388\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_yaml = Dcnn.to_json()\n",
    "with open(\"model.yaml\", \"w\") as yaml_file:\n",
    "    yaml_file.write(model_yaml)\n",
    "model.save_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2831b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
